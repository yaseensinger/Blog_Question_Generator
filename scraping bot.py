from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import csv
import openai
def get_version():
    """
    Gets the version of the Selenium library that is installed.

    Returns:
        The version of the Selenium library.
    """
    return webdriver.__version__

def openwebsite(driver):
    "Opens the website to the page with all blog posts listed"
    driver.get("https://investinginai.substack.com/archive?sort=new")




def scroll(driver, wait_time=0.5, scroll_pause_time=0.5):
    """
    Scrolls to the bottom of the page incrementally.

    Parameters:
        driver: The WebDriver instance.
        wait_time: Time to wait for new content to load after scrolling.
        scroll_pause_time: Time to wait between scrolls.
    """
    # Get the current height of the page
    last_height = driver.execute_script("return document.body.scrollHeight")
    
    while True:
        # Scroll down to the bottom
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        
        # Wait for new content to load
        time.sleep(scroll_pause_time)
        
        # Calculate new height and compare with last height
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break  # Exit the loop if no more content is loaded
        
        last_height = new_height
        time.sleep(wait_time)  # Additional wait for safety



def link_list_gen(driver):
    try:
        # Wait up to 15 seconds for elements to appear
        WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a[data-testid='post-preview-title']"))
        )
        time.sleep(3)  # Extra sleep to ensure elements are loaded
        
        # Find all elements that match the selector
        title_elements = driver.find_elements(By.CSS_SELECTOR, "a[data-testid='post-preview-title']")

        links = []
        for element in title_elements:
            link = element.get_attribute('href')
            if link:
                print("Link found:", link)
                links.append(link)

        if not links:
            print("No links found. Double-check the CSS selector or the page content.")
            
        return links

    except Exception as e:
        print("Error waiting for elements:", e)
        return []
    
def blog_extract(driver, links):
    """
    Extracts and saves the blog text from each link.

    Args:
        driver: The Selenium WebDriver object.
        links: A list of blog post URLs.

    Returns:
        A dictionary where the keys are URLs and values are blog contents.
    """
    blog_contents = {}

    def save_as_csv(blog_contents, filename="blogs.csv"):
        with open(filename, 'w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            writer.writerow(["URL", "Content"])
            for url, content in blog_contents.items():
                writer.writerow([url, content])
        print(f"Blogs saved to {filename}")

    print("......extracting .......")
    for link in links:
        try:
            driver.get(link)  # Navigate to the blog post page
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.body.markup"))
            )
            time.sleep(3)  # Wait for content to fully load
            
            # Extract the blog content
            content_element = driver.find_element(By.CSS_SELECTOR, "div.body.markup")
            content = content_element.text

            # Save the blog content
            blog_contents[link] = content
            print(f"Extracted content from {link}")
            
            # Optionally, save to a file
            with open(f"blog_{link.split('/')[-1]}.txt", "w", encoding="utf-8") as file:
                file.write(content)

        except Exception as e:
            print(f"Error extracting content from {link}: {e}")
        
    print(blog_contents)
    return blog_contents


def save_as_csv(blog_contents, filename="blogs.csv"):
    with open(filename, 'w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["URL", "Content"])
        for url, content in blog_contents.items():
            writer.writerow([url, content])
    print(f"Blogs saved to {filename}")

def generate_questions(blog_content):
    """
    Generate insightful questions for a blog post using GPT.

    Args:
        blog_content (str): The content of the blog post.

    Returns:
        List of questions generated by GPT.
    """
    prompt = (f"Read this text [Blog] and come up with insightful, thought-provoking questions "
              f"for a Podcast interview with Rob May - 4x Founder. Multi-Stage Tech Investor. 100+ AI investments, "
              f"Started Backupify, Talla, Dianthus, and BrandGuard. Some big wins and some big failures. "
              f"University of Kentucky grad and huge Wildcats fan. GP at PJC for a while, now a Partner at Half Court Ventures, "
              f"a fund that focuses on AI. Co Founder at the AI Innovators Community https://ai-innovators.com/. "
              f"I also write the Investing In AI newsletter at investinginai.substack.com and am an angel in 100+ early-stage companies, "
              f"mostly AI companies. Please don't contact me about anything crypto or blockchain.\n\n"
              f"Blog: {blog_content}")

    try:
        response = openai.Completion.create(
            engine="davinci",
            prompt=prompt,
            max_tokens=150,
            n=1,
            stop=None,
            temperature=0.7,
        )
        questions = response.choices[0].text.strip().split('\n')
        return [q for q in questions if q]

    except Exception as e:
        print(f"Error generating questions: {e}")
        return []

def save_as_csv(blog_contents, filename="blogs.csv"):
    with open(filename, 'w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["URL", "Content"])
        for url, content in blog_contents.items():
            writer.writerow([url, content])
    print(f"Blogs saved to {filename}")



def generate_questions(blog_text):
    prompt = f"Read this text [Blog] and come up with insightful, thought-provoking questions for a Podcast interview with Rob May - 4x Founder. Multi-Stage Tech Investor. 100+ AI investments, Started Backupify, Talla, Dianthus, and BrandGuard. Some big wins and some big failures. University of Kentucky grad and huge Wildcats fan. GP at PJC for a while, now a Partner at Half Court Ventures, a fund that focuses on AI. Co-Founder at the AI Innovators Community https://ai-innovators.com/. I also write the Investing In AI newsletter at investinginai.substack.com and am an angel in 100+ early stage companies, mostly AI companies. Please don't contact me about anything crypto or blockchain.\n\nBlog: {blog_text}\n\nQuestions:"
    
    # Send the prompt to GPT (make sure to replace 'YOUR_API_KEY' with your actual API key)
    response = openai.chat.completion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ]
    )
    
    questions = response['choices'][0]['message']['content']
    return questions.strip()

# Function to save blogs and questions in CSV
def save_blogs_and_questions_to_csv(blog_contents, filename="blogs_with_questions.csv"):
    with open(filename, 'w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["URL", "Content", "Questions"])
        for url, content in blog_contents.items():
            questions = generate_questions(content)
            writer.writerow([url, content, questions])
    print(f"Blogs and questions saved to {filename}")
    
def main():
    """Main function"""
    version = get_version() 
    print("The version of the Selenium library is:", version)
    
    # Initialize the Chrome WebDriver
    driver = webdriver.Chrome()
    
    openai.api_key = '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'
    
    try:
        # Open the website
        openwebsite(driver)
        
        #scroll 
        scroll(driver)

        # Extract the links
        links = link_list_gen(driver)
        # extracts the blog data
        blogs = blog_extract(driver, links)
        # save as csv
        save_as_csv(blogs)
        save_blogs_and_questions_to_csv(blogs)
        # Keep the browser open for a while to review (adjust time as needed)
        time.sleep(10)
        
    finally:
        # Close the browser after operation
        driver.quit()


        

if __name__ == "__main__":
    main()
