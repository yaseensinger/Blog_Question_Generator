URL,Content
https://investinginai.substack.com/p/would-you-use-an-outside-ai-program,"Happy Sunday and welcome to Investing in AI. I’m Rob May, co-founder of the AI Innovator’s Community, which does events in Boston and New York so please sign up if you are interested.
I’ve been thinking a lot today about how AI business models, like turning services into software ,might impact how we think about core competencies in business. In classic business strategy we outsource things that aren’t core if they can be done more cheaply by someone else, and we keep things in-house that help define our core business model and value proposition.
It seems that the last wave of tech innovation sold software that aided work, but wasn’t core to most companies. Building a CRM, for example wasn’t something most companies would benefit from doing internally. But AI is different. If you consider Sarah Tavel’s “sell work, not software” piece as a viable future for AI (and I do) then the question becomes, when you sell “work”, what will people outsource and what will they not?
The difference between AI and what came before it is, AI is more algorithmic. And algorithms can be more core. A high frequency trading firm doesn’t usually sell their software to others because it’s more valuable to own it and use it yourself to trade. I think AI might take more companies in this direction. What I mean is, as more things become algorithmic and AI driven, and software can do more human tasks - can you “sell work” or, will people consider it too core?
If you outsource your core competencies to someone else, particularly a 3rd party AI company, then that tool is getting better and regardless of what it does or doesn’t do with your data, it may enable more people to compete with you. So the question is - when a company comes to you with an AI product that can help automate something you consider core, do you buy it?
There are two possible ways this can be resolved in the future. One is, yes, companies buy it, and the source of competitive advantage changes to areas not yet affected by AI. The second is, no, you do it in house. This latter outcome is the one I’ve been thinking about and what it means for investing in AI. I find it difficult to wrap my head around this so let’s try with two examples that come to mind.
If you were at Bain, Mckinsey, BCG, or some place similar, would you buy a “consulting agent” from a software company? If you were a PE firm would you buy an analyst agent from a software company? If every part of your value chain can be done with AI eventually, is it safe to offload it all to third parties? It probably depends on how valuable the “stringing it all together” part is to your customer base. But also, this would make “stringing it all together” easier for everyone else.
I’ve honestly been grappling with where the key vectors of competition and competitive advantage are in an AI world. I’m not there yet but, I’m leaning towards the idea that AI will level the playing field for areas like operations and innovation. I keep coming back to the things AI can’t change, and I think, counterintuitively, those may be some of the best places to invest to make money off of AI.
The fixed capacity in the modern world is attention. It seems like commanding someone’s attention is the most valuable thing to have or be. That generally cements incumbent positions, except in cases of extreme novelty. Maybe the AI people are right, and attention is all you need."
https://investinginai.substack.com/p/is-ai-too-expensive-to-justify-the,"Happy Sunday and welcome to Investing in AI. I write a lot about all the opportunities in AI but it’s important to keep a balanced perspective so today I want to talk about some of the challenges in AI.
The biggest challenge, by far, is around the ROI of adoption. From what I’ve seen at my own company and those I’m invested in, lots of AI projects are stuck in the perpetual pilot phase as people figure out whether the technology is good enough and the problems it solves valuable enough to generate real ROI.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
Goldman Sachs recently published a newsletter that is the most skeptical assessment of AI ROI that I’ve read. Below is the most interesting excerpt:
The cost to develop and run AI technology means that AI applications must solve extremely complex and important problems for enterprises to earn an appropriate return on investment (ROI). We estimate that the AI infrastructure buildout will cost over $1tn in the next several years alone, which includes spending on data centers, utilities, and applications. So, the crucial question is: What $1tn problem will AI solve?
Replacing low wage jobs with tremendously costly technology is basically the polar opposite of the prior technology transitions I’ve witnessed in my thirty years of closely following the tech industry. Many people attempt to compare AI today to the early days of the internet. But even in its infancy, the internet was a low-cost technology solution that enabled e-commerce to replace costly incumbent solutions. Amazon could sell books at a lower cost than Barnes & Noble because it didn’t have to maintain costly brick-and-mortar locations.
The bolded statement above from the report rings true with my experiences in the market. Historically, tech lowers the cost to do things. With many AI applications, that isn’t the case, because AI is so expensive to run.
As an example, for BrandGuard to do an equivalent job checking a marketing asset for brand compliance as what a human would do, requires 22 different AI models. For all of those models to run in less than 60 seconds, which is a SLA similar to what a human would provide, takes several GPUs, which are expensive. And to do it quickly requires us to keep them up and ready to go even when not in immediate use. It’s not cheap to score a marketing asset with AI. But it is more scalable. Is the value of scalability worth the cost? The market hasn’t decided yet.
The question then, for investors, is how to place smart bets on things where AI can show strong ROI and ignore things where it can’t. But this is a moving target so we have to do this while factoring in that the models are constantly improving (thus providing the ability to automate more tasks) and the costs to run a model at a given performance level are decreasing (thus increasing the areas of available ROI), all while paying attention to the difficulties of dealing with AI’s jagged frontier. It’s a very challenging problem.
One reasonably safe bet though, is to look at technologies that lower the cost of AI training and inference, because those technologies will directly impact the ROI calculations of everything sitting on top of that part of the tech stack.
I’ve been a big proponent for years of a coming AI explosion, but my views have become more tempered as I’ve watched this festering ROI problem continue to grow. I still believe AI is the most important technology to ever be invented, but rolling it out through the economy is going to take much longer than I anticipated.
Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/the-things-that-ai-wont-change,"Happy Sunday and welcome to Investing in AI. I’m Rob May and I run the AI Innovator’s Community in NYC and Boston.
For my topic this week, I’ve been inspired by Morgan Housel’s book Same As Ever, which looks at the things that don’t change over time even as technology advances. It inspired me to think about AI through that lens. So much is going to change as AI rolls out across industries. But, what won’t change? What things are either outside the impact of AI or, even if AI can impact them, get rejected in favor of non-AI approaches?
One area I see is the idea of human relationships. Summarization is a constant theme of AI and that includes summarization of human relationships. There is an emerging trend of bots that respond to things for you, including other people. But if your bot talks to me to keep me updated on your life, but then I deploy my bot to chat with you and your bot, and our bots talk to each other, are we really building a relationship? Are summarizations of key life milestones without the actual interaction the same?
To take an extreme example - a summary of a sexual experience isn’t the same as the sexual experience itself, and in many cases of human relationships, the situation means a lot more than just the basic transaction of information. I generally don’t invest in AI that tries to summarize or replicate human experiences.
A second area where AI will have little impact is areas where there is no standard amongst humans. For example, I see a lot of startups that want to use AI to validate truth online. But if humans can’t do it, how can AI? A significant part of the population still believes there was fraud in the 2020 U.S. Presidential election. Having an AI tell them there wasn’t isn’t going to change their point of view. While there are things AI can do to help in this space, like finding and tagging AI-generated content, or monitoring and aggregating information about what’s going on in the news, it boils down to this - when humans can’t agree on truth, AI can’t help.
The third and final area I’ll suggest as an area AI won’t change things is areas that are regarded as status symbols. I wrote about this in 2019 in What Horses, Watches, And Bookstores Can Teach Us About Why Automation Won’t Kill Jobs. Here is an example from that post:
Two hundred fifty years ago, everyone owned a horse. That’s how you got somewhere. When cars came around, it didn’t kill off the horse industry altogether. Instead, horses became expensive status symbols. To own a racehorse, or to participate in equestrian events, it it’s own culture and community filled with mostly wealthy people.
In situations where something is, or may become, a status symbol, there may be limited chances to apply AI. AI works best when the goal is automation, or lower cost predictions, or something like that. Status symbols don’t usually benefit from either of those use cases.
It’s common to extrapolate new technology to everything. But it rarely works out that a new tech upends every area of the economy. Charlie Munger says then when analyzing something it’s important to “invert, always invert.” One way to think about where AI goes and the impact it has is to think about the areas that might stay the same even in the face of AI advancements. If you have areas you’ve thought about that AI can’t touch, I’d love to hear them.
Thanks for reading."
https://investinginai.substack.com/p/attention-is-all-you-need-but-not,"Happy Sunday and welcome to Investing In AI. I’m Rob May, and I run the AI Innovator’s Community in New York and Boston. Sign up if you want to attend our various events.
The modern AI wave was set off by a paper titled “Attention Is All You Need.” It’s a famous paper in AI technology circles but today I want to approach that same statement from a customer perspective.
The last 15 years of online marketing have exploited new ways to reach customers. First it was just being online with a website, then it was online ads, then search ads and SEO, then social media, and eventually mobile app stores. New waves of startups were able to challenge incumbents using these new channels that the incumbents didn’t understand. However, I think those days are mostly over. (One exception - agents could be a new channel but I think incumbents will dominate them)
One of the challenges with such a noisy world is that you need more than just an ability to “solve a customer problem” or “offer a benefit, not a feature” or “tap into a customer’s job to be done"" and all the other startup aphorisms that are supposed to get you to the growth stage promised land.
Let me explain why with an analogy. Let’s say I’ve eaten 3,000 calories already today in good, but not great meals. And then I have a chance to get a 50% off deal at the nicest restaurant in NYC. I may not go. Why? Because I’m already satiated. The marketing team for the restaurant will have a difficult time understanding why I turned them down. I have to eat, right? And they are the best, right? And they are offering a once-in-a -lifetime deal, right? What idiot wouldn’t take that?
The thing that is missing from the equation of all these startup is that concept of satiation. When you translate that into selling software, satiation equates to attention. To sell software, attention is all you need, but it’s the hardest thing to get.
I get email offers EVERY DAY in my inbox for things that I should want. People offer to lower my insurance costs, my financing costs, my HR costs, my marketing costs, and my development costs. People offer me the chance to generate more leads, which I would like to do, and they offer it as a free trial. I turn down all of them. Why? Because even though I have a problem that you are offering to solve, and your product or service has product market fit, and you are taking all the risk out of it for me to try it out, I still DON’T HAVE TIME. I need at least 15 minutes to understand it and think through it. It probably takes a little time to setup. I have to explain to my team why we are adopting this, and I don’t have time for that.
What is getting ignored in this current startup world is that we have so much !@#@% jammed into our limited attention spans because of a million notifications a day and a follow-up NPS survey for every single restaurant/software/service/product/travel experience and a million other things occupying our attention, there is no room left even for good stuff.
So the problem for startup founders is, you can have product-market fit, you can execute flawlessly, you can take the risk out of the buying process, you can offer a deal, and you still may fail because we live in a world where you are just… crowded out. You did everything right. You did everything your VCs said, but I just don’t have the mental bandwidth to pay attention to you.
And that’s just for low friction products. A lot of AI products are higher friction, so the situation is even worse.
Which brings me back to my original point - attention is all you need. The companies that win now are the ones that can find ways to get attention. I’d even go so far as to say attention-market fit may soon become more important than product-market fit. Building the 4th best product in the category but being able to get attention to it might be a better path to success than building the best product in the category and struggling to get attention. A decade ago I would have told you that those things, product quality and attention, were correlated but today I don’t think they are. (side note as an example, I’m 0 for 7 on apparel bought from instagram ads… it all sucked. I bought “the world’s most comfortable shoe” from instagram and it’s horrible.)
The problem for startups is… you know who has a lot of attention already? Incumbents. Building attention before you launch a product may become more important than building a product before you get attention, because for startups, attention is all you need.
Thanks for reading."
https://investinginai.substack.com/p/lessons-from-filterworld-and-a-warning,"Happy Sunday and welcome to Investing in AI. Today I want to talk about the excellent book Filterworld: How Algorithms Flattened Culture. It’s the kind of book techies hate because it implies that tech can make things worse, and for most of us, techno-optimism is our religion. (For the record, I wrote a few years ago about why tech keeps getting expected outcomes wrong, and my religion is best described as “everything is a tradeoff”)
But let me grab your attention with a stat from the book that you may find shocking. In 2023, Netflix streamed 4,000 titles. But in the video store days, a Blockbuster “super store” would have stocked over 6,000 titles. This goes against everything the internet was supposed to be. It was supposed to enhance diversity, options, the long tail. Remember? But in his book Kyle Chayka presents ample evidence that companies like Netflix are actively pushing the content they want on us under the guise of “personalization.”
Another story from the book discussed a research project where multiple Netflix accounts were created and different content viewed to meet some personality stereotype. While their recommendations diverged in many ways, all accounts were pushed “The Fast and the Furious” series, which it turns out, Netflix had paid a very high fee to license. This is not how we expect these things to work.
The internet was supposed to increase democracy in every area. We would have better politics because of more informed voters. We would have better journalism because we removed the gatekeepers and let people write what they wanted. We would have a more generally educated population, more appreciative of diversity, and with more exposure to the long tail of all types of content. But let me ask you something - do you think the average person you know is smarter, more engaged in democracy, and exposed to more diversity than 20 years ago? I don’t. And Filterworld shows how the same forces that homogenized thought are homogenizing culture. (For what it’s worth, I wrote about a related issue in Venturebeat in 2013. It was easy to see this coming.)
I’m really worried AI is going to make all of this worse. I hear all the same things about AI that I heard at the beginning of other waves of tech about how it will make things better. Yet, I don’t hear any acknowledgement that we made mistakes that time around and now have ideas how to improve with the next wave of innovation.
If you are working in the space, I hope you will read Filterworld, and I hope you will consider the implications of your work. Everyone seems worried about AI safety and AGI but, as Filterworld shows, we are doing a lot of harm to ourselves and our society with algorithms even now, well before we solve AGI. Let’s not lose sight of the more pragmatic issues our AI work can cause.
Thanks for reading."
https://investinginai.substack.com/p/the-valley-of-ai-innovation,"Happy Sunday and welcome to Investing in AI. This week I want to talk about how things have slowed down in AI, and why, to me, it feels like 2006.
Tomasz Tunguz wrote recently about the slowdown in software markets. It’s real, and I’ve seen several other people writing about it. In one good example, Molly White recently asked if AI is worth it. It feels like a wave of “AI is overhyped” is coming.
With the exception of foundation models and a few hot AI companies, things seem to be slowing down, both on adoption and financing. It’s easy to think, at a time like this, that we are entering some valley of AI that we may not get out of for some time. But to me, this feels a lot like 2006.
I remember talking to a friend in 2006 saying it felt like there was nothing innovative coming. It was a time when there didn’t seem to be any big breakthroughs, and companies were raising on stories like “we use AJAX,” or “we are a platform to add tag clouds to websites.” The platforms that were popping up, things like Reddit and Facebook, felt like toys. None of it felt like real innovation.
But just a few years later I remember pointing out to that same friend that the reason it felt slow was, we moved from a technical innovation period through a user behavior change period. Adoption was slow and uncertain. No one knew what to do with these things, and the tools hadn’t hit any kind of mass adoption. Yet bubbling under the surface was something big.
I don’t know what is next. Maybe it’s agents, maybe it’s edge AI. I’m doubtful it’s AGI. But something is brewing and in 2 years I think we will what the right theses were for investing in this phase of AI, and what types of companies are starting to win. In some ways it feels quiet, but it’s a very exciting time.
Thanks for reading."
https://investinginai.substack.com/p/why-i-dont-invest-in-agi-its-not,"Happy Sunday and welcome to the Investing In AI newsletter. I’m Rob May and I run the AI Innovator’s AngelList syndicate if you are interested in early stage AI deals. I’m also looking for some applied AI people to interview for the AI Innovator’s Podcast, so if you work on implementing AI tools into corporate workflows, please reach out. I’d love to discuss it on the podcast.
If you make AI investments, you inevitably see a regular flow of companies who list their corporate mission as “solving AGI” (artificial general intelligence) or something like that. I usually don’t take a second look at these businesses - not because it isn’t an important goal, but because I don’t think it’s a good economic opportunity. That sounds crazy right? What could be more valuable than building the world’s first machine that is as intelligent as a human?
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
— The Singularity —
Let’s first look at the core theory behind why AGI is perceived to be so powerful. It’s often called “the singularity.” There are many perspectives on this but they generally looking something like this:
The first machine to reach human intelligence will then start improving itself on its own, without human intervention
Having even a brief head start of a few hours or days will make it a winner-take-all outcome as the first truly intelligent machine will take off on a self-improvement curve so that no one can catch up
An AGI machine will be capable of so many things that it will dominate many areas of economic activity.
— AGI Economic Skepticism —
I have to admit that years ago, in older versions of this newsletter, I was a believer in the singularity and I thought companies and academic labs might be in a race that mattered immensely to the future of the world. But after almost a decade now of working at the intersection of AI and business in various forms, I no longer feel this way. Here are my main arguments for AGI economic skepticism.
The gap between the top AI groups is small, so I’m skeptical that the first group to solve AGI will do something that others can’t figure out and copy quickly.
It’s not obvious or guaranteed that the first machine to reach AGI will also be the best at self improvement. Humans obviously don’t know exactly what to do to build AGI, and so building a machine with human levels of intelligence doesn’t guarantee that machine will quickly find the next breakthrough for the fastest form of self-improvement. In fact, you can imagine a scenario where the second company to build AGI targets a better self-improvement algorithm and quickly passes the first.
Even if a company gets to AGI with a significant lead, they may be gated by compute or power resources. For example, if OpenAI reaches it first then both Microsoft and Google, with vastly more compute, may be able to catch up and pass the OpenAI AGI as it could be growth constrained by compute.
It’s unclear how dominant a digital AI could be on the world. We don’t have a model of a disembodied intelligent being to know. And we don’t know what motivations it would have, so, it could just as easily be that we end up with an AGI oligopoly as the first AGI is limited in its influence and growth. Interacting in the physical world is still slow and difficult.
If you do have an AGI, that doesn’t mean everyone will buy it and you will be economically dominant. Large enterprises are particularly sensitive to data and privacy issues with generative AI. They will be the same, probably even more stringent, with AGI. So it’s unlikely that just because you have an AGI suddenly every large company adopts it. The adoption will take time, and that’s time for others to catch up and compete.
The market will want different AGI options. Some companies won’t want to use certain providers, depending on who they are. Walmart won’t use AWS, and when we are looking at workflow intelligence over just compute, that issue will be even more serious.
We don’t know the upper limit on the theoretical value of AGI because we don’t know anything about the upper limit of intelligence in a fundamentally probabilistic world. The theoretical maximum might be not too far beyond where humans are today.
As you can see, there are a lot of practical reasons to believe that the achievement of AGI isn’t some magical point of singularity where the winner dominates the world going forward. I think it’s an important goal, but not necessarily one that will be a massive economic return.
In fact, while many AI investors spend cycles focused on what capabilities GPT-5 may have, or whatever next model is going to be dominant, I’m much more focused on the billions or trillions of daily workflows around the world that don’t yet use any AI technology. I believe pushing GPT-4 level AI deeper into these workflows is more valuable than building the next GPT-X. AI will make so many workflows so efficient and drive so many productivity increases, and I think the economic gains could go to the companies that adopt AGI more than the providers of AGI themselves.
The mental model I use is this - as intelligence becomes more abundant and everyone has more access to it, what goes up in value? I think it’s industry knowledge, customer relationships, unique data, and experience with the nuances of specific industry workflows that aren’t easily available to an AGI.
I’m sure many of you have counterpoints I haven’t thought of. I’m always happy to hear them so please reach out if you disagree.
Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/what-will-be-the-soc2-of-ai-models,"Happy Sunday and welcome to Investing in AI! I’m Rob May, co-founder of the AI Innovator’s Community. If you want to join our investing syndicate, it’s not up are running here. Our first deal is a very cool AI agent platform called Wayfound. You can apply to invest in the deal at this link.
I’m also looking for beta testers for a new startup I’m invested in - Silvershield. It’s a tool for adults to manage their aging parent’s phone/email/etc in a world where deepfakes and AI scams are getting more serious. Please give it a try if you are in that demographic.
My thought for today is on how slow it is for enterprise AI companies to sell products. We’ve seen this at BrandGuard, and many other companies I’ve invested in. It isn’t a lack of interest. It’s a fear of what might happen with your data when you adopt an AI tool.
A normal enterprise procurement process is slow. We’ve seen an average of 3 months on top of that just to evaluate an AI tool. Why? There isn’t a standard evaluation checklist and people don’t know all the questions they should ask. So what happens is the technical group asks for demos, architecture diagrams, etc, and asks a ton of questions as they think of them. It reminds me of selling cloud software in 2009, when businesses didn’t know what questions to ask of cloud providers.
The cloud computing market eventually standardized on key questions and an evaluation process for buyers. Part of that was the use of third party verifications and certifications like SSAE-16, ISO-27001, and SOC2. What we need is a similar certification process for AI companies.
There are some groups trying this but nothing seems to have any traction yet. While the market is probably a little too early to settle into a single certification, I think something will evolve in the next 24 months that is like a SOC2 for AI models. It’s the only way large enterprises can have certainty and offload risk of AI adoption.
This isn’t just an issue that affects the initial buying cycle though. As we use models to automate more and more tasks, how do you audit them to make sure they are doing what we expect? Take a tool like Waze, which helps drivers find the fastest route to their destination using AI. How do you actually know it’s providing the shortest path? I sometimes ignore Waze and turn somewhere else and the estimated time to arrival drops two minutes. What if a similar thing happened for an AI powered business workflow?
If you have a fully AI powered resume screening tool, how do you it’s working to give you the candidates you want? If you have an AI powered sales tool, how do you know it’s really giving you the best recommendations of which deals to focus on? Models suffer from data drift, and their performance often deteriorates over time. How will you know when that is happening unless you have a process to audit and test it against something else?
Compliance, governance, audit, and certification functions will be necessary to make AI adoption happen at scale for these reasons.
So what types of issues will these certifications likely cover? I think a few key things:
Data uses for training models for individual customers and across all customers
Established ownership rights for various levels of model outputs
Disclosures of third party models, APIs, and other tools that may have data access
Indemnification standards for various violations
Data governance workflows and processes - both technical and human driven
Oversight and clarification of any Human-In-The-Loop workflows
Ongoing audit and testing processes
Possible parallel workflows or constant A/B testing of algorithms to see which ones perform better.
This is a really interesting issue to me, having experienced it on multiple levels in my own startup and others. While everyone is watching the key tech breakthroughs in foundation models, it’s really the gaps in compliance and fit with existing workflows that are holding back AI adoption. Watching the state of those things is a better gauge to where applied AI is going and a better guide of where to invest, in my opinion.
If you are a big company executive working on this, and have ideas for how these standards may develop, let me know. I’d love to hear your ideas.
Thanks for reading."
https://investinginai.substack.com/p/why-ai-could-lead-to-higher-costs,"Happy Sunday and welcome to Investing in AI! Today I want to talk about an unusual idea from an old book I picked up recently. I often browse the basement of Strand Bookstore in New York because it has the best selection of used technology and mathematics books of any place I’ve seen. Recently I found Edward Tenner’s book “When Things Bite Back: Technology And The Revenge of Unintended Consequences.” I bought the book because I’ve been a long time believer in the idea that the tech industry is too naive about the tradeoffs, unintended consequences, and second order effects of new technology adoption. But what is most interesting about this book is… it was written in 1996. I wanted to read it and see if we are still making the same naive mistakes in our assumptions about technology.
One of the book’s later chapters talks about all the ways office workers became less productive in the age of the computer. It works like this… executives used to have secretaries to type, print, and copy things, but now with all the computers and software and tech innovations, the executives can do these things themselves quickly and no longer need secretarial help. So the support staff in the company can be mostly let go, saving a lot of money.
But what the research showed is that it actually ends up costing more. Cal Newport came across the same book several years ago, and wrote about this same study here.
Here is Newport’s analysis of it all:
This reduction in the typical deep-to-shallow work ratio (see Rule #1 in Deep Work) became so pronounced as computer technology invaded the front office that Sassone gave it a downright Newportian name: The Law of Diminishing Specialization.
What makes Sassone’s study particularly fascinating is that he used rigorous data collection and analysis methods to answer the question of whether or not this diminishing specialization was a good trade-off from a financial perspective.
His conclusion: no.
Reducing administrative positions saves some money. But the losses due to the corresponding reduction in high-level employees’ ability to perform deep work — a diminishment of “intellectual specialization” — outweighs these savings.
And here are two interesting quotes on the results from the original study:
“The results of a comparison of a ‘typical’ department, with a department with a reasonable high level of intellectual specialization were startling. The typical office could save over 15 percent of its payroll costs by restructuring its staff and increasing the intellectual specialization of its workers.”
“The typical office can save about $7,400 [around $13,200 in 2018 dollars] per employee per year by restructuring its office staffs and improving its levels of intellectual specialization.”
It’s easy to bah humbug these results and make a lot of excuses for why this is no longer true. And maybe to do so would be accurate. In fact, the Consensus research tool shows that most papers say yes, new tech has increased productivity. But skimming these papers, it seems that they analyzed employee per unit performance rather than system level performance. What Sassone seems to be saying is that, while individual employee performance on certain tasks might be more productive, the system overall may not be because of the loss of intellectual specialization benefits.
Whether this is true or not of the web 1.0 era is not my concern here. My concern is whether or not some of this could apply to the AI revolution. To understand that, we need to think about it on a case by case basis, and understand how good an AI is for particular use cases.
For example, consider automated support systems. They are more efficient and much cheaper than humans for routine requests, but my anecdotal experience with them is that complex requests that used to be handled quickly by a human are now more difficult. You have to go through all the basic menus and validate a bunch of information to get transferred to a human who can actually help, even when you know you have a complex topic that will require a human.
I wrote a few months ago about how evaluating the downside risk of an AI project could give you insights into the ROI. That’s the case I’m worried about. If too many AI projects are “pretty good” but have to be constantly re-checked by humans, need more human supervision, or result in more complexity in other areas, then the gains of AI could be lost.
We could also find ourselves back to the diminishing specialization problem. I’m not a designer, but if I feel like I can get 90% of what I want from a foundation model on my own in 20 minutes, I’ll probably do that more often. To me, it seems better than having a designer take a few hours on the same thing. But then, that’s time I’m not spending doing what I do best.
I know most of you who are techies are pretty strongly against this point of view. You believe almost religiously that tech improvements are always good and should always be adopted and are always beneficial if we just do it the right way. Yet second order effects in tech are powerful. For example:
Uber makes traffic worse, despite early tech predictions it would make everything better.
Social media makes us less connected, despite assumptions it would help us maintain connections.
AI adoption in many areas of business is slow. Why? Because many of the experiments companies are running don’t show enough value when you look at them holistically.
What is the story we will write about AI in a decade? Will it have transformed everything we do? Lead to increased productivity? Created a bunch of negative second order effects? My guess is that it will look a lot like most other areas of technology. AI will have pockets of huge benefit and productivity, while net ROI may be negative but we will be reluctant to acknowledge it. But I’ve been wrong a lot before so, perhaps this won’t come true.
If you have opinions on this, as always, I’d love to hear them.
Thanks for reading."
https://investinginai.substack.com/p/the-factors-impacting-nvidias-defensibility,"Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Brandguard. I also co-founded the AI Innovator’s Community that does events in New York and Boston, so please check it out.
Today I want to talk about NVIDIA’s run up and if/when it might stop. The growth and profits are obviously determined by two things - general growth in the demand for AI and competition from people who aren’t NVIDIA.
I started my career in the chip industry as a FPGA and ASIC designer, so I know a bit about the industry first-hand. It has some very unusual industry characteristics that make it hard to determine if and when NVIDIA might be unseated as the major hardware producer for AI. Let’s review these characteristics in two parts. First, the criteria people look at when choosing a chip to use in their product. Then secondly we will look at factors in the way design cycles, market demand, and competition impact how chip companies grow and develop.
Before we dig into it, there is something you should understand about AI and computer chips. There are many different types of AI chips beyond GPUs. There are new ideas like neuromorphic chips, analog chips, or chips that merge semiconductors and biology to do intelligent computing. They are very different than GPUs but can apply to some of the same AI workloads. But also, you can run AI workloads on CPUs as well, they are just slow. So beating NVIDIA isn’t necessarily about building a better GPU. It’s about disrupting GPUs for certain AI workloads.
When we hear the news about AI chips, it typically focuses on benchmarks for performance on certain popular AI models. You can see one set of rankings for that here. Performance overwhelms the decision making on AI chips at this point in the market because these are mostly large chips for server farms for training large models, and AI is still in the nascent stages of being deployed inside many products, particularly at the edge.
As the applications of AI grow, managers looking to build AI products will evaluate chips on many different things beyond performance, including:
Footprint (how large is the physical chip and can it fit in my device or use case?)
Power consumption (really important for battery or low power devices)
Price (always)
Tolerances (heat, vibration, exposure to radiation, space, military, etc)
History (how long has the company been around, is this a brand new chip unproven in the market?)
Roadmap (this matters A LOT in the chip industry)
Tool chain (how does the chip get programmed and integrated and does it fit existing design workflows)
Use case generality (do you need a more general or more specific targeted chip?)
These variables make it really really difficult for a newcomer to break into the market because given the longer chip design cycles, you have to take a bet on what the demand is going to look like down the road for what you are building. So as you can see, NVIDIA probably won’t dominate across all of these vectors, although it is very strong on the most important ones.
That leads to my second point, which is the design cycles are long and that matters because it’s difficult to see far enough in advance to get things right, and can take a while to recover if you get things wrong. Just consider that Intel passed on providing chips for the iPhone launch because they didn’t expect it to be a large business.
That’s a funny thing about how the chip markets work. The big growth areas usually happen because new markets emerge for existing chips - not because chips are designed for market use cases as they are evolving. Consider that GPUs were not developed for AI, and not a use case for NVIDIA customers for most of the company’s history. But when AI arrived, GPUs were the best bet.
As another example, ARM specialized in low power chips for decades before the mobile phone revolution. And for those of you around in the early days of mobile phones, you probably remember their eventual ubiquity was not obvious. I worked at Radio Shack in the late 1990s in college and were highly commissioned for selling phones, so I pushed them any chance I could. But most people would say “what the hell would I ever do with a cellular telephone? What’s so important that I can’t wait until I get home, or to a payphone?”
So getting back to NVIDIA, I don’t see any force stopping the juggernaut in the next few years, other than a huge decline in the need for AI compute, and that seems unlikely. Any startup competitor could easily be acquired by NVIDIA for tens of billions of dollars and it would barely be material given NVIDIAs market cap. And while the other major chip companies are mounting a challenge, particularly AMD, the demand for GPUs is so great, and will be for the next few years, there is room for massive growth for multiple players.
When NVIDIA eventually slows down, I suspect it will come from one of two areas. The first is a surprising shift in market use cases. Given the long development cycles of computer chips, it’s hard to be as responsive to the market demands as software companies can be. And product designers have to choose some level of predictability and reliability in their components and their partners, which is why they favor proven tech over innovative new designs most of the time. But the way NVIDIA benefited because their chips for graphics cards happened to be a good match for new AI workloads - I think something similar could happen to slow them down. Some new popular AI workload will be a better fit for something other than a GPU, but a chip that is already established in the market in other ways.
Or secondly, it comes from a company that is building their own chips and decides to get into the broader semiconductor business. This means Apple, Amazon, Facebook, or most likely - Google. But all those companies, even if they can compete technically with NVIDIA, lack the infrastructure to sell and support chips and those who develop them. Building those capabilities will be slow.
Overall I think NVIDIA will remain dominant for quite some time and I don’t see any impending forces that could slow that down. But we know it won’t go on forever, so it’s fun to speculate where the eventual new king of AI will come from, and how far into the future that might be.
Thanks for reading."
https://investinginai.substack.com/p/how-i-think-about-ai-investing-in,"Happy Monday and welcome to Investing in AI! My last post caused a bit of a stir, and generated the most email feedback of anything I’ve written in the past couple of years. (Evenly split between agree and disagree.). But the most common question I got asked is where I’m investing now, and where I think the opportunities are. Today I’ll attempt to lay out my thinking and answer that question.
First of all, I didn’t intent to imply in my last post that there are no early stage opportunities. (I define early stage as pre-seed and seed in that post, but realize many investors may include a few later rounds in “early stage.”). The post was just highlighting that the combination of changes in the venture ecosystem plus the rise of AI and it’s unique properties might be changing the shape, size, and number of early stage opportunities in ways that make investing there more difficult.
The second things is that, while there will definitely be some early stage opportunities that remain, I think the bigger opportunities, in aggregate, are not in that market segment. I believe, as I stated in the previous post, that publicly traded tech companies have become better at defending against the innovator’s dilemma, so the place to go next is markets that haven’t had to defend against it so far.
If I had to list out the key drivers in my thinking on where to invest next, I’d lay them out like this:
The “jagged edge” of AI makes experimental approaches better than analytical approaches.
Contrary to the conventional wisdom that startups are the most experimental, big tech companies are very experimental and can run experiments longer and dedicate more resources to AI experimentation because the gains will be larger.
The cost to develop software is going to drop dramatically over the next 5-7 years, changing build vs buy decisions and making it hard to build a “traditional” SaaS company. Actually, let me correct that statement. It will make it so easy to build a traditional SaaS company that competition will erode margins. You don’t want to invest there.
The traditional online methods of customer acquisition are going to weaken due to the rise of agents.
AI in the short to mid term still relies heavily on data, data, data, and many existing companies have (or can collect) unique data sets easier than startups can.
I think a lot about technology changes through the lens of economic complements. I use the mental model that two things are linked and one drops dramatically in price, increasing demand, so demand for the other goes up but, maybe AI doesn’t affect the production/supply of that thing as much. It’s why we built Brandguard. I was thinking about the explosion of marketing content generative AI creates, and then, what will be in more demand as a result? — The review and approval of that content.
Given this lens, here are things I currently think about when investing in AI:
Don’t focus on stage, focus on insights about AI, and apply them to whatever stage is appropriate. This maps to the jagged edge concept of AI - good and bad applications of AI aren’t always obvious so when you find one, lean into it at whatever stage it best applies.
Find business models that are more than software, because pure software will become less defensible over time. More and more I like models that bundle software with something physical: sensors, services, a needed human touch, etc.
Consider companies that can benefit from strong ecosystems, particularly in GTM. The SaaS wave of the last 15 years was weak on channel partnerships as an early GTM focus. I think that will change. If you have a lot of channel partners or other integration and ecosystem partners who benefit from your success, there are many people who are incentivized to make sure you aren’t replaced.
Look to apply AI in industries that have not had to defend much against the innovator’s dilemma. This could mean bigger more vertically integrated business models but, in many cases I think that will be the right play.
To add some clarity to where I stand on investing in AI - I think the opportunity is massive. But unlike other types of technology that came before, it won’t all start with startups and expand up market. There will be opportunities everywhere, but I think pre-seed and seed AI companies that look like the pre-seed/seed companies of the past decade, will not be nearly as successful.
As always, thanks for reading."
https://investinginai.substack.com/p/changes-to-investing-in-ai-why-im,"Happy Sunday and welcome to Investing in AI! I’m Rob May, CEO at Brandguard. I also run the AI Innovator’s Community in Boston and New York. If you have a chance, please check it out. Also, until recently, I was a very active AI angel investor. Today I want to talk about why I’m moving away from that, and what to expect in terms of changes in this newsletter.
I sold my first company in December of 2014 and committed $1M to angel investing over the next few years (and eventually more). I made my first angel investment in June of 2015. Since then, if you factor in direct angel investments, my time as a VC, AngelList deals, etc… across all platforms I’ve made 133 investments in 9 years. Ninety percent of them were AI related, and almost all of them were sub $1M in revenue. Most had not revenue. And I’ve been the very first check of any kind into about 20 deals.
The returns have been pretty good. I’ve had two unicorns so far and a few more that are close. I didn’t realize how long it takes to start returning capital from early stage investments so, I was a bit too aggressive in the beginning but, now I have a few exits every year and it’s pretty balanced.
What I loved, and still love, about early stage, is the fight that entrepreneurs have to go through. It is really exciting to watch, to see the future being built, and to mentor and encourage first time founders, or commiserate with repeat founders.
But in the 18 years I’ve been involved in startups, early stage venture has changed, and I think there are many negative forces pushing on early stage startups that make it very frustrating as a place to play. On top of that, I’ve grown to lack respect for a lot of VCs. I don’t want to paint any industry with a broad brush because there are a few thousand people working in VC worldwide and many of them are great. I’ve had the chance to work with some great ones. But by and large they don’t really believe what they say they believe. Here are the things I don’t like about early stage VC.
They invest on momentum, not conviction. This leads to more fads getting funded than solid companies. Real businesses with competitive advantages take time to build. That’s not what I see mostly get funded. In fact, if you can’t pop quickly with some gimmicky trick, it’s harder to get funded.
Salaries and hiring practices haven’t adapted to the market. When I started in startups, an engineer might have to take a 20% pay cut to work for a startup, for the chance to get rich from equity. Since then, big tech company compensation has far outpaced startup compensation growth, and VCs haven’t adapted. Instead of asking someone making $120K to go to $100K, you are now asking someone in their 30s making $400K - $600K to go to $180K. And the truth be told, their big company equity is probably going to be worth more. VCs will lecture you about being a cog in a machine or whatever at a big company but, the truth is it’s harder and harder to hire great people because you have to ask them to take a 60%+ pay cut. I’ve seen too many of my investments get stuck with mediocre talent because it’s all they can afford.
Tech companies are better at defending against startups and have bigger new project budgets. Many public software companies were once venture backed startups. That wasn’t the case 20 years ago. These companies are familiar with the innovator’s dilemma and are constantly snapping up good teams, small pre-revenue startups, and are funding all kinds of projects to make sure they don’t get hit by a surprise innovation. They are better than public companies were in the past. On top of that, a new project at a big tech company might get a multi-million dollar annual budget for a few years to prove something out. But as a startup, you have to show more progress and momentum on your $1.5M pre-seed in a year or less. It’s unreasonable.
Goodhart’s Law Plus Too Much Seed Money Has Ruined Things. There has been an explosion in pre-seed and seed funds in part because you can raise these from HNWs and Family Offices and don’t need formal institutional money, which makes it easier. But there isn’t enough follow on capital, and the small funds can’t take you that far. Goodhart’s Law says that “when a measure becomes a target, it ceases to be a good measure.” Early stage startups never ask me about business building. And they sure as hell don’t have the time or money to invest in building a durable competitive advantage. They ask “what do I need to get a Series A” and they build towards that. If you wonder why so many companies get to Series B, then see intense competition, slower growth, or even collapse, it’s because they were trying to satisfy Series A investors (a la Goodhart’s Law) rather than build a real business.
Many of the most interesting tech problems are at big companies now. Years ago, most interesting tech was at startups. But for AI I’m not sure that’s true. The AI startups doing the most interesting work have all raised massive amounts of money - they aren’t small at all. Outside of those, much of the most interesting AI work is at Google, Facebook, Amazon, and that tech co-hort. Maybe as a startup you have a shot at getting someone to take a pay cut to come work on something incredibly intellectually stimulating. But now, big companies have most of those problems. It’s actually more fun there in many ways.
Too many companies make progress but have tired syndicates. This ties back to the rise of so many small funds. I’ve been invested in a lot of companies that pivot a time or two, finally figure things out, but end up with a tired syndicate. New investors don’t want to do a recap, so the founders have all this knowledge, have finally figured out some key insights from 2-3 years of working in a market, and it just dies.
Early stage investors are slow to adapt to new business models and realities. When I was raising for my first SaaS company in 2009, a significant proportion of investors were anti-SaaS and still wanted companies to measure bookings and maintenance contracts and such. I was told SaaS was a fad that wouldn’t work because it took too much capital. Now as AI companies are changing those metrics (I see most a lot monetizing more like infrastructure companies - usage based) VCs are still trying to wedge them into a SaaS metrics framework for evaluation.
Again, I’m talking in general trends I’ve seen and am painting with a broad brush. There are a lot of great investors who don’t have these issues I mentioned above, and there will always be market opportunities at early stage. But I believe the market dynamics are not in favor of venture over all, and will get worse in the next few years.
As a result, I’ve paused most of my angel investing. I’ll still do deals that I really love, but only because I personally love something about the tech or the team. I’m not going to keep looking at 1000+ deals a year, because I think the big opportunities, particularly in AI, are elsewhere.
Plus on a personal level, I’m tired of working on early stage problems. It’s not fun trying to convince people of an idea they aren’t familiar with, and it’s stressful to constantly have the existential risk of dying. It’s much more fun to think about the kind of real business strategy you get into when you have competitors, lots of revenue, multiple products, multiple channels, and sell around the world. I have my own startup to get to that level, and that’s enough for now. (No I won’t do another startup from scratch either - I’ll buy something next time and start there.)
This newsletter is pretty personal, as all my blogs and newsletters have been over the years. I don’t write to monetize, or even to build an audience. I write for me actually, not you. I don’t use ChatGPT or anything else because my goal isn’t just putting out content. My goal is to use writing as a process to think through and refine my ideas. That doesn’t work if I take shortcuts.
Over the coming months, expect more of a shift to PE and public markets ideas about AI. I’ll still write a lot of the philosophical stuff, but it will probably be less early stage focused. And there is a good chance I’ll start building in more decision making and leadership talk. Those are two topics I’ve been deeply interested in for years, have written a lot about in other formats, but that have come back to the forefront of my mind with some things that have happened recently. The business world and political world both lack the kind of leaders who believe in ideas bigger than themselves, and I believe this is a bad thing for the world.
The newer stuff won’t be right for some of you, and that’s ok. Feel free to unsubscribe and find something you enjoy more. In the meantime, thanks for reading."
https://investinginai.substack.com/p/just-in-case-ai-does-downside-risk,"Happy Sunday! I’m Rob May, CEO at BrandGuard, the world’s only AI powered brand governance platform. I’m also an active angel investor, and run the AI Innovator’s Podcast. Our 2024 season is kicking off in two weeks so if you know a good guest please reach out.
Today I want to write about what happens when AI moves beyond human capacity, and whether or not we should have any concerns or backup plans. I’ll use BrandGuard as an example and talk a bit about how supply chain lessons from Covid are relevant.
At BrandGuard, we ingest brand guidelines and sample content and then use it automatically approve or disapprove customer facing brand assets. So far we haven’t seen any customers roll GenAI out at scale so all of our use cases are for companies that just have sprawling marketing groups and produce enough content with humans that ensuring brand consistency is already a problem. But everyone we talk to is considering GenAI and figuring out how to use it.
In that world, BrandGuard becomes even more valuable because if you are making 1,000 personalized emails or 5,000 landing pages or 10,000 custom videos with GenAI, using humans to ensure they are all brand compliant is not possible. You have to lean on AI. That means many assets with customer facing messaging will go out without human approval.
As society advances in rolling out AI across more and more processes, this “no human checked this” issue is going to grow. That might be fine in most cases and will definitely make most workflows more efficient. But we know there are always exceptions, surprises, and unexpected issues in tech. What happens when, for whatever reasons, the machines fail? And they fail at scale?
For brand assets that may be a problem, but it won’t crash the world. For some areas where AI may be applied, it could cause major problems.
I’m reminded of just-in-time supply chains. When I was in business school, it was one of the big things we talked about - how Japan crushed the U.S. in the 70s and 80s because they adopted JIT. By the time 2019 rolled around, just-in-time supply chains were the norm pretty much everywhere.
Then Covid hit and messed all that up. Everyone wished they had a second source of supply, or had more inventory on hand, or whatever. Supply chain people stopped talking about “just in time” supply chains and started talking about “just in case” supply chains.
My question is - how do we apply that to these AI workflows we are building? What does a “just in case” AI workflow look like, and what does that mean for how we set it up? Should humans be sampling and auditing some AI decisions? Will other AIs monitor the first set of AIs? Will we need human backups for everything?
The economics of rolling AI out to many business and government workflows will sometimes be driven by the economics of the “just in case” piece. When AI lowers the cost to perform a task by 90% because you offload it to machines, having a human backup may be a significant add-back. It might be enough of an add-back to not make the cost of the initial rollout worth it.
This world will be upon us very soon, where AI breaks in surprising ways, in places we didn’t expect it to, and people will start talking about “just in case” AI backups, and will be stuck in that case until AI gets human level or better at that task. That could be a while. Think about self-driving cars. It was about 8 years ago we were hearing that fully autonomous driving is just around the corner. Now I don’t think very many people believe it will be here in the next few years. The expected economics of full automotive autonomy have changed.
I wonder what fields will most need a “just in case” AI backup, and how that will make for less attractive AI economics in some surprising places.
Thanks for reading."
https://investinginai.substack.com/p/the-coming-dark-patterns-of-botomation,"Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at BrandGuard, and co-founder of the AI Innovator’s Community. I’m also an active early stage investor (most recently Graphlan and Salessync).
Today I want to write about the dark patterns of the internet and how those are likely to get worse with AI. Dark patterns seem to be increasingly common to me. A simple example is how Amazon defaults to buying certain things via subscription, rather than one-time purchases, and you have to modify that choice before you buy. Subscriptions benefit Amazon. That’s why they are the default. I’m sure internally someone has crafted some arguments that defaulting to subscriptions benefit the user, but they are mostly like specious and self-serving.
Notifications are another dark pattern. I’ve written in other places about my distaste for the Doordash notifications. If you let them be in their normal state, you get about 15 per order:
Your order went through to the store
They are working on your order
Your order has been picked up
Your delivery person is approaching
Your order has been dropped off
For each of these notifications, you get it 3 times: email, in-app, and text. If you modify them and shut them off, you end up with just 6 notifications, I believe. And that’s if you “turn off” notifications. Someone from Doordash responded and told me why. The hard part of the Doordash model is the delivery drivers. If you don’t notice your delivery happened, and get it 15 minutes after it’s been sitting outside, your food is cold and you are mad and if I remember correctly, maybe the driver gets rated low on the delivery quality, which affects their pay.
So Doordash nags you to death with notifications to avoid that problem. It’s a dark pattern because it’s more beneficial to Doordash and delivery drivers than to customers.
I bring this up because, I believe AI will create dark patterns far beyond what we’ve seen today. In particular, I expect the botomation that is coming - everything turning into a bot - will contribute to this. Why? Because automated bots are great at gathering information that companies will want. Thus, the incentive will be for the bots to nag you, and since it isn’t a human who actually has to feel bad about it, it will be easier to justify because no human is in the loop to complain.
Think about how annoying it is that Alexa is always asking extra questions and offering stuff you don’t need but that Amazon wants to force on you. I just ask for today’s weather and Alexa tells me something about a new artist releasing an album I like or something I bought in the past is now on sale. I just wanted the weather.
As bots take over everything, they will have so much information at their fingertips, and the incentives to gather more. I’m sure bot designers will justify it by saying things like “imagine if it can just anticipate your thoughts and give you want you want before you even know it.” Call me old-fashioned but, I like thinking. I don’t want something to short-circuit my thoughts and predict everything for me. I already live my life in ways that try to minimize my reliance on algorithms (I personally hate recommendation engines of all types) and I dread the world that botomation will most likely create.
I’ve written before how tech constantly creates problems that weren’t expected. We obsess over lowering the friction to certain activities, missing the idea that maybe friction is sometimes a good thing. And we ignore possible second order effects of our decisions. Anytime you think about something that’s a neat hack, you should think about what life is like when everyone does that hack at scale. If you are designing bots and thinking about the future of tech, I hope you will think about more than how to manipulate users to keep up engagement and growth in data sets. Businesses should think holistically about their products and their impact on the world. That’s about best chance of avoiding the dark patterns of the coming botomation.
Thanks for reading."
https://investinginai.substack.com/p/the-agent-economy-and-economic-growth,"Happy Sunday and welcome to Investing in AI! I’m Rob May, CEO at BrandGuard. I also run the AI Innovator’s Community, so if you are in NYC or Boston, we have tons of events this year. Check it out.
Today I want to look very far ahead at a possible problem for the economy that AI could create, and ask how we may deal with it. Imagine these AI agents we are building get better and better. Over time they take over more and more tasks from our work lives and personal lives. As this happens, the instructions we give them will get ever more abstract.
What happens to agents at this point? They will start to learn more, and become more personalized. If one person runs a small business and the other just has standard W-2 income, their agents will need to know different things. If one person owns a home and another rents an apartment, their agents will need to know different things. You can see a world where agents can buy knowledge modules. I can buy the “small business tax” module or the “home repair” module for my agent if those are relevant to me and not to other people.
Now imagine my agent can also package up things it’s learned and sell those to other agents. This could happen a lot in business because so many situations are unique. For example, when I sold my first company, we had started as a LLC and converted to a C corp when we took venture capital. It was unclear if the LLC time frame pre-conversion should count towards the holding period QSBS, and our accounting and law firms went deep to figure that out. Imagine that knowledge was in an agent that had gone through the process, and you could purchase that knowledge.
Once this happens, you can see an economy growing among agents. Agents that collect unique interactions and data sets will sell them to other agents. Agents with broad goals (e.g. “help me grow my business”) will go out and find some sources of knowledge that they believe they need to achieve the goals they are given.
What happens to the world economy as the agent economy grows? At 5% of all economic activity it probably doesn’t matter, but what about at 50% of the world economy?
Much like individual humans, the agents will maximize their own best interest, not the good of society, which means if their demands for knowledge create an economic boom that leads to inflation, they won’t scale back on their own. Or, maybe the opposite happens and the low cost of knowledge and the ability to do everything cheaply and electronically leads to deflation, which causes it’s own set of problems.
We can imagine what will happen by looking at other markets that have become more electronic over time. Take high frequency trading as an example. HFT firms will argue they provide liquidity and help make markets run more smoothly but, lots of research shows they increase volatility and aren’t really beneficial to markets.
When AI agents run increasingly more of the economy, what will it mean for us?
Maybe AI agents will be regulated by the Fed. Maybe growth rates will be constrained by Fed (and their international peers) rules, and programmed into agents so that their ability to grow will ebb and flow with general economic trends, acting as a counterbalancing force to other economic activity.
There are so many ways this could play out that it is difficult to imagine exactly how the economy will work when we hit this point. But, it’s a real possible future, and one that we should be thinking about.
Thanks for reading."
https://investinginai.substack.com/p/2024-the-year-of-objective-function,"Happy Sunday and Happy New Year! Welcome to Investing in AI. I’m Rob May, CEO at Brandguard, and active AI angel investor. (If you want to contribute something to the newsletter, I don’t take donations or subscriptions but I am looking for an introduction to Elad Gil, and I’m sure one or more of you knows him well.) Let me know if you can help me connect.
I’ve been thinking a lot about what’s next in AI. A lot of stuff has already been written on what to expect in 2024 so I’ve been trying to think of what may have been missed. My answer is - objective function engineering.
When we build algorithms, particularly AI algorithms, they need a goal. This goal is usually expressed as an objective function - what is the AI algorithm trying to accomplish and how does it know if it’s doing a good job? So far, these objective functions are pretty straightforward.
I often analyze AI problems by asking “how would humans accomplish this?” and when you think about objective functions - humans have a hierarchy of them. Ultimately our genes want us to reproduce. But to do that, we have to stay alive, which requires an objective of finding food and water. We also have to stay safe and keep from dying, which is another objective. Sometimes to do that we have to play nice with the social structure if we live in one so, we have that as an objective. Those objectives all have to be satisfied first, and only then can we focus on the objective of finding someone to mate with. Our objective function hierarchy is complicated.
AIs on the other hand, have pretty simple objective functions, so far. Over the past few years, the pieces of an AI stack have started to grow in complexity and variation. We moved from one giant model to ensembles of models, in particular “mixture of experts” models. Then we went from simple prompt engineering to more varied and complex prompting. I think in 2024, objective functions are next.
When we think about building AGI, there is still clearly something lacking from even these LLMs like Bard and ChatGPT, compared to humans. What could it be? Well, I think these models are all trained on smaller objectives. Their objective functions are simpler and less complex, and not hierarchical. In 2024, that will change.
To build AGI we have to endow a machine with more complex goals. Machines have to have meta-goals, like “stay alive” and “learn” which can be vague and difficult to define as objective functions. And shorter term goals like maximizing performance on certain tasks that lead to those broader goals. Maybe a short term goal is to find a new data set for training, for example. But I think this kind of new objective function engineering is the path forward to a new conceptual step in building AGI.
So what do I expect in 2024? I expect a lot of work to be done, and a lot of interesting papers and posts to be written, about objective functions. I expect to see more vague and varied and hierarchical objective functions. Maybe someone will create “situational objective function embeddings” where, somehow they create embeddings of possible objective functions that change for an AI based on its situation. I don’t know. I’m speculating. But, I do expect this to be the next area of focus and a big initiative for 2024.
As always, if you have thoughts on this, or have worked on something related, I’d love to hear from you. Thanks for reading."
https://investinginai.substack.com/p/introducing-graphlan-ai-powered-usage,"Happy Sunday and welcome to Investing In AI! I’m Rob May, CEO at Brandguard. I also run the AI Innovators Community, and AI Innovators Podcast. Today’s post is different from my usual fare. Today I want to focus not on an industry trend or idea about AI but a practical application by a company I helped co-found (I use that term loosely - it’s about a 1-2 hour commitment a week for me as others have done almost all the work).
About a year and a half ago, I was talking with some friends about the problems of LinkedIn. It really should be an open API so that people could build whatever applications they want on top of it, and users could take their network to any application they use. Your network should be more like a utility, and some company should manage that utility for all apps that want to use it, to control the rules of interaction and keep it up and running at scale for everyone. The problem that keeps coming up is - how do you get started when you don’t have the network?
We found a use case in founders and early investors. So I want to introduce Graphlan, which is debuting this week with it’s alpha launch. Here is what makes Graphlan unique:
It ingests your contacts via LinkedIn, and/or Gmail, and provides an AI based search that works much better than other things you can find on the market. It does well for use cases like “who do I know in New York that has AI experience and has worked in the Fintech industry?”
It allows you to add custom metrics to user profiles that only you see, and can search on. I can create a metric like “entrepreneurial” and score users myself, then add that term to a search to make it even more effective and build better data sets for the AI.
It is super easy to create and share lists of contacts, to create joint lists, and to manage those contact lists. This is useful for planning events, running a founder led sales process in an early stage company, or finding co-investors for an angel around. It’s my favorite feature and I’ve been using it personally for weeks now.
And finally and most importantly, the model is to allow an open API where anyone can build on Graphlan provided they follow the rules and don’t spam. We believe users should be able to bring their network to any application, as long as that application doesn’t abuse access to the network. Instead of getting the crappy LinkedIn experience you get with one sales navigator or recruiting tool built on top of the network, there can be dozens built on Graphlan, allowing different user experiences, vertical market targets, and more. This competition should make the whole ecosystem of your business network better. While the API has not launched yet, you can request early access if you tell us about your app.
Graphlan is the best tool I’ve used in years and has already become a staple of many network related workflows for me. The data it’s collecting should power some interesting new features in the AI related business network space.
The company is raising a pre-seed round, and hiring! So if either is of interest to you, please reach out. But most importantly, I hope you will go try out the tool and let me know what you think.
Thanks for reading."
https://investinginai.substack.com/p/things-ai-cannot-scale,"Happy Sunday and welcome to Investing in AI! I’m Rob May, CEO at BrandGuard. I also co-founded the AI Innovators Community in Boston and New York. Check it out if you want to join. We just did a great startup showcase event in Boston last week, and have a full slate of events in 2024.
We are entering what feels like a golden age of AI automation, and as a result I see early stage startups trying to automate pretty much everything. One of the most time consuming things we do, as humans, is communicate with each other. Some of the startups I’ve seen are trying to automate various workflows often pitch communications automation.
For some types of communication that’s fine. Customer support is really transactional. I just want to get my problem solved and be done, so AI is well suited to this. But I’m less convinced of some of the other use cases I see, like automating the communication aspects of financial advisors, hiring processes, or managing people.
Some processes are not meant to scale. In particular, a lot of person-to-person actions are required to build trust and smooth over the bumps that naturally occur in human relationships. Can AI automate these away?
Imagine you have a good friend, but one who can be a little… demanding on your time. What if you could deploy a bot that sounded like you, but would text the friend automatically? That would be all well and good until you ran into them on the street and they commented on the conversation and you had no idea what they were talking about. Or, you might ask them a question to be polite to which they answer “we discussed that over text last week.”
Now imagine we all use bots to maintain our relationships. My bot talks to your bot. We never really know if it’s us or the bot talking. But - are we really maintaining a relationship when we do that?
I think it’s important to understand the limits of AI to invest in it properly, but it seems like most people are just assuming AI will automate away pretty much everything. I don’t think that’s true. Processes that are more relational and less transactional are going to be more difficult to offload to AI - now matter how good it gets. In these cases, AI is a better assistant than automator. We should focus on that distinction as we build and invest.
Thanks for reading."
https://investinginai.substack.com/p/how-the-fundamental-limits-of-intelligence,"Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at BrandGuard. I also run the AI Innovators Community in Boston and New York, and we have a startup showcase coming up November 30th in Boston. There will be speed dating between big companies and AI startups, and tables for startups to demo. If you are a tech executive wanting to meet startups, or a startup wanting to present, or just an AI practitioner who wants to attend, sign-up here.
This week I want to talk about a topic that I don’t hear many people discussing. The question I want to ask is - are there fundamental limits to intelligence? And if there are, what does that mean for investing in AI companies?
This is important because there is a lot of talk about superintelligences and what they will mean. Is it a race between the big tech companies to be first? Is it a race between governments and countries to win at designing the first superintelligence? But a bigger question is - why do we think this is even possible?
Humans tend to extrapolate linearly in almost every field of knowledge. That is our intuitive sense about how the world works and how systems scale. Thus our baseline assumption is that as compute gets more powerful and data sets grow, so will our abilities to build smarter and smarter machines.
On top of that, all the data so far shows that larger foundation models perform better as they get bigger. They show emergent skills. A model that doubles in size performs more than 2x better than the smaller model. This is a reason to believe a superintelligence is possible.
But.
Building a superintelligence machine will ultimately hit the limits of physics in some areas. Circuits can only get so small before you are down to just a few copper atoms in a wire that carries the current through the circuit. Transmission and computation can only happen so fast as they bump up against the speed of light and other limits of physical materials. (Ignoring quantum computing for now).
To take a side detour as an analogy, look at what Bryan Johnson at Blueprint is doing. He is trying to optimize his health using all the latest research to halt and reverse aging. But what is most interesting to me is that, it seems he is starting to hit limits where he has to make tradeoffs. The way bodies are designed, optimizing for endurance running and optimizing for powerlifting are in conflict. I don’t think it’s possible to be world class at both, even if you could genetically engineer a human to attempt to do so. Johnson is working through this with the tradeoffs he must now make by figuring out what he wants to optimize for. Doing everything possible to minimize your chances of avoiding one type of disease might open you up to increased risk of others.
So the question I want to ask is - if there is no way to make a human body perfectly healthy across every dimension, because some optimizations conflict with other optimizations - will the same be true of building an intelligent machine?
We know there are biological limitations to human intelligence. Are there theoretical limits to generalized artificial intelligence as well? Ancedotally, it seems to me that the more tasks Amazon Alexa does, the worse it performs on any given task. Is this the future of intelligent machines in general?
These limits show up in most places in the world, not just biology. For example, in corporations, there are economies of scale but also diseconomies of scale that stem from complexity, beyond a certain size.
Intelligence is composed of many different components. Can we really build a machine that is the best at all of them?
To sum it all up, I believe there is a possibility threshold of intelligence beyond which you must optimize for some components of intelligence (or maybe for specific types of knowledge) because you can no longer optimize for generalized overall intelligence and push all the major dimensions of it forward.
What will this mean for investing?
It means if this begins to happen, investors, entrepreneurs, and executives, will need to understand the tradeoff boundaries. In other words, where is it best to make tradeoffs, to what end, and how? How do the fundamental limits that drive these tradeoffs map to use cases, technologies, data, and other aspects of the intelligence supply chain?
If this is true, it means building a superintelligence may not be a winner-take-all game.
I don’t know if this will happen. But I wanted to bring it up because it’s something I have discussed with friends but, haven’t really heard much of a public discussion about. They say chance favors the prepared mind so I believe giving some thought to issues like this before they arise might make us more prepared as investors if they do.
If you have opinions or ideas on this, I’d love to hear them, as always. Thanks for reading."
https://investinginai.substack.com/p/ai-not-blockchain-is-the-real-path,"Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Brandguard. I also run the AI Innovator’s Podcast, so go listen if you are into that.
Today I want to talk about blockchain. I had coffee with a crypto fund manager a few weeks ago who mentioned that, for all the money that’s gone into it, there is still no killer blockchain app. I know him because I spent a lot of time in blockchain in the past. I first bought bitcoin in 2014, and I tried to launch a token in 2017 before getting yelled at by the SEC and deciding not to do it.
My take on blockchain is that the reigning story about why we need a blockchain is flawed. The idea that decentralization is/would be a good thing is pretty pervasive in tech. I don’t know for sure but I believe it comes from how we were raised as Americans to believe so strongly in individual liberty, and America’s disproportionate influence on the tech industry in general. I’m not sure if decentralization really is always a good thing but, for purposes of this post let’s assume it is.
The question to ask then is, why aren’t things more decentralized? Why did centralization arise? The crypto industry’s answer is that it was about trust. As commerce expanded beyond your local geography, you needed large institutions you could trust. And thus, if you can have trust without centralization, we can solve the decentralization problem.
I’m going to make a different assertion. I think centralization was about scaling intelligence. As the world became industrialized, there were economic benefits to scale that ultimately tied back to scaling intelligence. The form of modern corporations enabled knowledge sharing and learning on a scale not possible before, and allowed a limited number of people with expertise in a subject to have a huge impact with what they knew. If intelligence and expertise on most subjects were widely available, I don’t believe we would have seen the economic centralization that ultimately emerged.
Now I realize many of you will dispute the above statement, and that’s fine. But assume for a minute I’m right. It leads to an interesting insight. It means that as intelligence moves from humans to computers because of AI, and thus becomes more scalable, it could lead to decentralization of many areas.
How? If I can have, for example, my own autonomous financial agent that runs locally on my phone that happens to be as smart as all the JP Morgan finance wizards together - well, it can be easily personalized to my needs and I can be in full control of it.
I believe that the mass personalization that AI can bring is going to lay the real groundwork for the decentralized economy that the crypto industry is seeking.
Where will this come from? To me, that auto-GPT movement is the most interesting thing happening in AI right now. Building individual agents that can swallow global knowledge at scale but learn locally from individual interactions has tremendous potential. It’s early, and there are many problems to solve for these agents to materialize. But when they do, it could be very disruptive to existing centralized economic models.
Thanks for reading."
https://investinginai.substack.com/p/how-to-pivot-an-ai-company,"Happy Sunday and welcome to Investing in AI! I’m Rob May, CEO at BrandGuard, and host of the AI Innovator’s Podcast. I’m also a very active angel investor in the AI space, and today I want to discuss some of the things I’ve seen work in the past for pivoting AI companies.
I made my first AI startup investment in June of 2015. It’s been over 100 investments later and I’ve been quite lucky that when those early AI companies failed, they teams and/or tech usually got picked up by someone who needed AI talent. A few have turned into nice outcomes. But now that AI is more pervasive, it won’t be so easy to land a failing company somewhere. You may have to pivot. In my experience, this is something many entrepreneurs do poorly, so I want to provide a list of questions to ask yourself if you are going through it that may help you find the right new direction.
If your direct model isn’t working, is there a channel model that works? It’s very common for AI companies to struggle to sell direct, but it’s the way most SaaS companies were built so most of the sales and marketing folks in tech these days are most familiar with a direct model. But I’ve invested in a few AI companies that struggled to sell direct but partnered with various types of solution providers as a channel partners and had good success.
What value does your data have? The most practical way to pivot is to find a good use case for the existing data. Consider whether you can combine it with other forms of data for some uncommon or non-obvious predictive value.
How valuable is your customer as a data generator? Some data sets related to certain types of people or professions are difficult to collect. If you built a good user base or deep customer relationships in a group like that, and they have some trust, consider what data they may have, or be able to generate, that you could get that others couldn’t
Is there a data equivalent of lead gen for your business? In the early days of the web, people made a lot of money building simple lead generation businesses. For example, build a mortgage calculator, and sell the user leads to a mortgage company. Building apps or games to collect data sets is a similar opportunity.
Can you be a plugin to some other app? VCs don’t generally back lightweight apps that could be features of other larger platforms, so you may have been initially discouraged from this. But that can actually be a great place to start if you can build off that in new directions some day.
In general, my advice is to pivot into your market learnings, not into what you’ve built. There is a temptation to always play into the sunk cost fallacy and say “we have this tool, who can we sell it to?” That works sometimes, but startups are really learning machines and there is a chance you’ve learned a lot. Do the harder pivot into what you’ve learned.
In fact, “pivotability” is something I consider in my angel investments. I go into it thinking the company is probably wrong about many things, and I try to make a guess for how easy it will be to pivot into other areas. Is the tech unique? Is the customer profile valuable and underserved? Is the team fast and flexible? If so, there is a good chance I’ll do the deal even if I’m not crazy about the initial idea.
Investing around pivotability has been wildly successful, and lead to some of my best investments. So don’t be afraid of the pivot. Many of the best companies have one in their past.
Thanks for reading."
https://investinginai.substack.com/p/jagged-frontier-the-best-new-idea,"Happy Sunday and welcome to Investing in AI! I’m Rob May, CEO at BrandGuard, host of the AI Innovators Podcast, and active AI angel investor. I don’t charge anything for this email and, if you like it, you can help support future writing by sending me an intro to Nike - it’s the one large company we haven’t been able to connect with at BrandGuard that we think would be awesome.
This week I want to write about the Jagged Frontier. Every once in a while an idea comes around and really makes you think about AI in a new way, and the Jagged Frontier fits that description perfectly. The paper that coined the term has a bunch of good stuff in it. It highlights the fact that yes, AI will have a huge impact on the way we work. I encourage you to go read the whole article about the paper.
Here is the key section that I believe is so powerful (bold is mine, not original):
AI is weird. No one actually knows the full range of capabilities of the most advanced Large Language Models, like GPT-4. No one really knows the best ways to use them, or the conditions under which they fail. There is no instruction manual. On some tasks AI is immensely powerful, and on others it fails completely or subtly. And, unless you use AI a lot, you won’t know which is which.
The result is what we call the “Jagged Frontier” of AI. Imagine a fortress wall, with some towers and battlements jutting out into the countryside, while others fold back towards the center of the castle. That wall is the capability of AI, and the further from the center, the harder the task. Everything inside the wall can be done by the AI, everything outside is hard for the AI to do. The problem is that the wall is invisible, so some tasks that might logically seem to be the same distance away from the center, and therefore equally difficult – say, writing a sonnet and an exactly 50 word poem – are actually on different sides of the wall.
The implication here is powerful. It means that the best way to understand the capabilities of AI are to explore the jagged frontier. There isn’t an analytical way to understand it, so you have to use an experimental way. Some companies, many companies in fact, aren’t built that way. Companies want to analyze things and make decisions about the path to pursue, but, according to this new research paper, that won’t work with AI. You have to try things, experiment, and explore.
So if you don’t have an AI beta program going somewhere in your company, you are falling behind, and unlikely to understand where the jagged frontier lies.
Thanks for reading."
https://investinginai.substack.com/p/hurry-up-and-wait-the-challenge-of,"Happy Sunday and welcome to Investing in AI! I’m Rob May, CEO at Brandguard. We teach machines to understand brands and branding and our product ties in a bit to today’s topic. I also run the AI Innovators Community in Boston and NYC, but hopefully it will come soon to your city too. And check out our podcast. We are always looking for new guests.
This week I want to talk about one reason AI adoption is slow. It’s the “hurry up and wait” challenge of AI workflows.
You may wonder why, with all the breakthroughs in AI - the cutting edge research papers, the LLMs, the cool new tech - why does it still seem like most of our lives are still not permeated by AI tools?
From where I sit as an AI entrepreneur and investor, the problem is that the advancements only affect one or two parts of most workflows. If it takes 7 steps to get a task done, and 2 of those steps are now rapidly better because of AI, all that may have done is create a bottleneck on the other side of the workflow. It becomes a “hurry up and wait” situation where you accelerate part of a workflow but then, just sit waiting on the rest of it.
This insight is actually what led us to the invention of Brandguard. If you’ve followed along with the whole story, we started off building a generative AI ad platform, but we ran into the issue of “hurry up and wait.” In many large companies, there are layers of approval and sign off, sometimes even legal gets involved, and so, if you use generative AI to speed up content creation, that content still just sits there for 3 days waiting on approval. That doesn’t really help you respond faster to market trends, news, or customer feedback.
On top of that, CMO after CMO told us that even thought it was cool we could create 5,000 flavors of an ad to make them hypertargeted and better performing, they didn’t have time to review 5,000 ads, and they wouldn’t send them out without a review.
That led us to realize that generative AI could only be adopted at scale we could teach machines to be the reviewers as well as the creators. And while some creation platforms are building in similar functionality, it just makes more sense to have a centralized and independent review process, so we dropped the generation functionality in favor of a focus on machine approvals for all customer facing content. When machines do the approval, the whole workflow can now move at AI speed.
As an entrepreneur, how do you play this space to avoid the “hurry up and wait” problem? You really have 3 options:
Wait on the rest of the market to provide solutions that can be matched together to accelerate the full workflow and all the steps. This is difficult because it makes the timing out of your control. But, if you focus on one of the final enabling steps when the rest of the market pieces are there, it could be a rocketship.
Do the full stack yourself. This gets around the hurry up and wait problem but, requires you to raise more capital and compete early across multiple product categories. It’s a great move if you can pull it off but, you also risk getting picked apart over the long term by best of breed point solutions.
Focus on the slowest step in a workflow. If a workflow has 7 steps and you can make the slowest step much faster with AI, there’s value there and you speed up the workflow even if the step after you in the workflow is still human-based.
Over the next decade, AI is going to transform most all workflows. That’s obvious. But investing in AI requires some insights into how those transformations will happen. How will the world move from the way something is done today, to the way it will be done in a decade? There will be lots of great product ideas that die along that path, because of the hurry up and wait problem. So be thoughtful about what you build and invest in, and try to minimize your chances of getting stuck in that hurry up and wait dilemma.
Thanks for reading."
https://investinginai.substack.com/p/beyond-pmf-ai-and-tech-as-a-fashion,"Happy Sunday and welcome to Investing in AI! I’m Rob May CEO at BrandGuard (yes we are rebranding the company to the main product name). I also produce the AI Innovator’s Podcast (reach out if you have a good guest suggestion), and am an active AI angel investor.
A few links this week… my friend Doug Levin has an AI Summer Reading post. My colleague Eric Koziol writes about whether or not AI can know our desires. And Erik Duhaime from Centaur Labs joined me on the podcast to discuss data labeling and AI.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
The problem I’ve always had with how we discuss product-market fit (PMF) is that we treat it like an opening in a jigsaw puzzle. If you just find the right piece that fits, it’s all good, and your job as an entrepreneur is to figure out what the market wants. This has not really been my experience as an entrepreneur or an investor.
I see the situation more like the fashion industry. There is a scene in the movie “The Devil Wears Prada” where Meryl Streep, playing a high powered fashion designer, is trying to choose between two belts that her assistant says look so different. Anne Hathaway, who plays an admin that is new to fashion, laughs because, as she says “they look the same.” Streep then gives a lecture on fashion, the gist of which is - you think you wear that lumpy blue sweater to show you don’t care, but in fact that sweater was chosen for you by the very people in this room. The point is that our choices are often influenced in ways we don’t understand, even when we believe they aren’t.
The scene is just 2 minutes but worth watching.
I bring up this issue because I think as tech has become more mainstream, it behaves more and more like the fashion industry. For example, I’m an Android user, and have been for a long time. I’m regularly showing off cool phone features to iphone users that they don’t have (Android has had almost every major innovative feature first), who usually aren’t aware that an Android can do whatever cool thing it is I’m showing. We discuss the feature for a bit and then they lament, “yeah that’s cool but I’m so hooked on Apple because of the design.” Choosing form over function - isn’t that the definition of the fashion industry?
I see this everywhere in tech, but I don’t think we tech people like to think of ourselves as this way. We are engineers and MBAs and thoughtful critical thinkers. We are independent minded and don’t make decisions based on what’s popular or seemingly cool in the moment. Except that, watching the tech industry over the past 20 years I’ve seen it, I think we do. Tech is a fashion industry.
The reason it’s important to understand this is because it affects how you approach company building. Finding PMF isn’t just about a static market with a hole that you can plug into if you build the right thing. It’s actually about a dance between companies and markets and users. You have to influence the market as much as you have to read the market, especially if the market is nascent. Early markets can evolve in many different directions and I don’t believe one is foreordained. The early companies influence how they evolve, and if you had different companies the market could have evolved very differently.
This is an important issue for AI companies because they are creating so many new use cases in so many new markets. If you are working on things like the agent or auto-gpt spaces, we don’t have established markets for those. Finding PMF will be partially about creating PMF.
In the fashion industry, you can’t make something that is really ugly and stupid into something popular. But from all the possible things that could be popular, you can influence what gets to the top. (Research has shown this with song popularity). I believe tech is the same way.
This isn’t to say you can build anything you want and force the market to take it. You can’t. But I’m pointing out that too many companies are trying to build for the market as if it’s a static thing with a hole for a new product opportunity, and that it’s just about talking to customers and finding the right thing rather than influencing how the market will play out. You can, and should, try to influence how the market develops, and few early stage companies make the effort to do so. Those opportunities will be plentiful in new areas of AI products.
Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/the-biggest-mistake-ai-investors,"Happy Sunday and welcome to Investing in AI! I’m Rob May, CEO at Nova. We make BrandGuard which is an ensemble of AI models that help with brand protection and governance in an AI world. I also run the AI Innovator’s Podcast. Let me know if you have a good suggested guest.
I know many of you like to see it when people take on my ideas publicly so I have two things for you this week. My 5 Contrarian AI Theses post generated a lot of comments, and two people were brave enough to challenge some of them publicly. Parasvil Patel from Radical Ventures came on my podcast to discuss his views on those points. And then Eric Koziol wrote a great piece analyzing and disagreeing with some of my points. Go check them both out.
This week I want to talk about what I believe is one of the biggest challenges when investing in AI. To understand this challenge, I need to make an important point first. That point is - knowledge and intelligence, and particularly the way they intertwine, is discontinuous. This point is evident in working on data science problems.
For example, sometimes in data science and AI you are trying to solve a particular problem and you start with one technical approach. That approach, through more data and slight tweaks, gets you closer and closer to your goal, but then you hit a wall. Maybe you need a model that is 94% accurate and you start at 60%, slowly climb through 70%, 80%, and around 86% you stall out. The model just doesn’t get better with more data. The approach may be at it’s limit. To make progress beyond the level of 86%, you have to start over with a new approach using a new data science technique.
There are two types of common discontinuities in AI. One is when performance jumps much more than expected - e.g. you double the amount of data you have but triple the performance of the output from that. The other is when you max out on one technique and need an entirely new approach to keep making progress.
The difficult thing about investing against these discontinuities is that we, as early stage investors, aren’t used to them. They don’t exist in most other realms of investment. If a company is building computer chips and is the leader at the 28nm processing node, they probably have the technical prowess, customer visibility, and capital to also be the leader at the 22nm node as technology nodes shrink. If a company is building B2B SaaS, say a CRM, the story is similar. Anyone who comes along as a challenger has to replicate may of the old features, and the scale of customer acquisition channels, that the original company has. So, assuming the original company will maintain it’s leadership is a solid bet.
Disrupting incumbents it’s hard because historically, it has required something to change in the environment to make the incumbent vulnerable. That could be a major technology change, a regulatory environment change, or a change in consumer behavior. Now, in an AI world, these tech discontinuities mean you can’t make the bets you made previously.
If you are in AI, think back to CNNs, LSTMs, and GANs and how revolutionary they all were? Today those technologies barely matter and everything is Transformer architectures. But even things built on transformers aren’t the end-all-be-all. AI still lacks something. Humans are still way better at most things, and we do it with less training data.
That tells me another jump is coming - a tech discontinuity. Something new will take us from LLMs to more logic and reasoning. I expect a new tech architecture breakthrough in AI.
If/when that happens, what does it mean to companies on the LLM path? Will the same companies be dominant in this new wave, or, will the new technology be sufficiently different to just rapidly destroy the value of all the old tech? Will companies building LLM-related infrastructure be able to adapt to it to the new thing, whatever that new thing is?
If AI were like other technologies, I would expect LLMs to have a 15 year life cycle of leadership. But AI is different. LLMs may be gone in 3-5 years. What does that mean for investing in companies in and around the LLM space?
I think to invest in the space, you have to look for companies that have one of 3 things.
Stable customer relationships that will give them time to switch out the underlying technology as it advances. Customers should see the company as a sherpa, guiding them through an AI journey, and not making decisions purely on technical prowess.
Creative and flexible teams will be really important. In a world where the tech could change with one new published paper or code release, teams can’t fall in love with one way of doing things.
Applications that use many different types of AI and machine learning solutions, so that the company is well versed in solving a plethora of problems and not tied to any one solution that could go away overnight.
The broader point here is - in the world of foundation models, market leadership that is based on technical leadership could be fragile. Be careful investing against that. It makes me wonder if OpenAI should be a short. The company that leads now may have too much stake in the status quo to see the next thing. Time will tell.
Thanks for reading."
https://investinginai.substack.com/p/ai-cannot-solve-qwerty-problems,"Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Nova. We teach machines to understand brands, and make BrandGuard, which is a brand safety tool for AI (I don’t charge for this newsletter so, if you want to help me out, please send me a Fortune 1000 customer for BrandGuard). I also run the AI Innovator’s Community, a group in Boston and New York that cuts across all other affiliations and brings people together just focused on AI topics. We are launching groups in other cities soon, so stay tuned. And we have a great podcast if you are interested in that. In our last episode, Parasvil Patel from Radical Ventures gave his views on my “5 Contrarian AI Theses” post. Check it out.
Today I want to talk about QWERTY problems. I’m talking about the history of the keyboard most of us use. History says that QWERTY is a keyboard layout that is one of the slowest possible ways to type, and that it was designed this way because typing too fast on older typewriters could cause the letters to jam if two were typed two closely together. (Those of you over 40 may remember this problem). Regardless of whether or not that fact is true, it’s definitely slower than a DVORAK keyboard on which all the world typing speed records have been set. Why haven’t we all moved over?
Adoption is a tricky thing, particularly when it comes to standards. There is a book that isn’t that popular but worth a read called Network Power that discusses how standards get set, and what they mean in terms of “power” dynamics. From an idealistic perspective you don’t have to necessarily adopt a standard. You can do your own thing. But the form of power that standards create usually makes non-adoption impractical. This network power is part of the reason adoption of new things doesn’t happen. The value of the network is just more valuable than the better way of doing the thing.
What does that mean for AI tools? Many of them require new ways of working. They require new value chain steps like data labeling, new mindsets like thinking of outputs probabilistically, new quality and control workflows to deal with the scale at which AI can make things happen, and more. The more entrenched and universal the standard, the harder it is to change.
From an investing perspective, this creates an interesting dilemma. On the one hand, you want to evaluate a startup’s path to adoption and you prefer that it be an easy one. But on the other, if long term profitability tends to come from business model defensibility, well, there are business models people don’t attempt because of this QWERTY problem. Taking them on, and trying the very difficult path of changing a standard, could lead to bigger better outcomes for those companies.
When I consider investing in AI startups that have the QWERTY problem, here are the questions I ask myself.
Is the established path dominant enough that it will discourage most entrepreneurs? If so, I like that. Less competition, although admittedly a lower chance of success.
How strong is the value prop of the new way of doing things? This has to be looked at through an individual adoption lens, not a TAM lens. I could easily find some stats on how much more productive a DVORAK keyboard makes people, and how much keyboard typing happens around the world in a year and it’s probably a TAM worth hundreds of billions in productivity. But that’s really a meaningless number.
How much have people tried to change the standard? There are some areas, like payments, where people try frequently to change things. I prefer areas no one has looked at for a while because very often, the way to change it hasn’t been obvious simply because it’s been 15 years since someone has thought to seriously try.
And finally, can the team pivot around a bit? To take on these types of challenges, you need very flexible thinking. I want to see a team that will find a creative adoption path - preferably one that isn’t obvious from a simple market analysis but one you only find from living in the market for a while.
I bring up this topic because AI is moving VERY FAST. And as often happens in these speedy markets, thorough analysis goes by the wayside. Stop and ask yourself about adoption paths, and how AI might take on standards based businesses, when you are looking at those opportunities.
Thanks for reading.
Rob"
https://investinginai.substack.com/p/what-does-it-mean-for-a-startup-to,"Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO of Nova. We teach machines to understand brands and used that to build BrandGuard. I also host the AI Innovators Podcast, and am an active angel investor. If you have an interesting AI startup, please reach out. I’ve done over 100 AI deals and been the very first check into 15+ deals over the past few years.
Today’s post is less about AI and more about startups in general. It was inspired by a conversation I had with a startup I’m invested in, where I ended up saying “I know some investors will tell you to focus more but, I don’t think you should at this point.” I realized that was uncommon advice and so I decided to really dig in and take a deeper look at startups and “focus.”
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
On top of that I was looking at my first batch of 40 angel investments from 2015 - 2018. Of those companies, 12 are worth over $100M, at an average entry point of $8M (the range was $3M to $20M). Of those 12 that have done really well, 10 of them were companies VCs hated initially and part of the reason was, they seemed “unfocused.” Those founders heard things that VCs say like “this seems like a technology looking for a problem to solve” or “if you say you are serving everybody then you are serving nobody.” I hate those sayings because they are bullshit. Markets and companies are all different. Going really broad is sometimes the right approach. And many great inventions didn’t have an obvious use case in the beginning, or at least, which use case to start with wasn’t obvious.
I really prefer these big broad messy ideas because they have large market potential and they tie into major changes in thinking over time, rather than these tiny rifle shot super focused ideas that so many VCs like that often end up getting stuck in a small corner of a market.
The problem with “you need to focus” is that focus sounds like a very specific piece of advice, but it’s not because focus can be applied at many levels. Let’s say you are building a new CRM. What does it mean to “focus”? Is it focused enough to say you are building a CRM for an industry vertical, say, manufacturing? I could always focus more. What about a CRM for manufacturing companies with $20M - $200M in revenue? That’s more focused. But wait, I could make it a CRM for manufacturing companies with $20M - $50M in revenue. That’s more focused right? And what about ones that primarily have field reps and need mobile? And what about geo-targeting just the southeastern united states? So now I have a CRM for manufacturing companies with $20M - $50M in revenue with a focus on mobile use cases in the southeast. Super focused right? But is the market too small?
If you are too narrowly focused, you may have a small TAM, and you may not be getting feedback that is indicative of the broader market. If you are too broadly focused, your efforts may be diluted in ways that make you mediocre at everything and great at nothing.
The real issue focus is trying to get at is the return on marginal dollar invested in the business. How does that next dollar spent move the company forward? Focus is a matter of degree that varies per business opportunity. Some market entry points require a broader scope than others.
What I don’t like about this discussion on focus is that people throw it around without any analysis. It’s used as if narrowing in your business model is always a good thing, but that isn’t true. I’ve invested in over 100 startups at this point in my career, and a common problem is that they get stuck in something too small. I feel like a rare voice encouraging them to think bigger, in contrast to the majority of their investors telling them to focus focus focus, which they interpret to mean do something more narrow. But in my experience, people use focus to mean “I think you should be doing something a little different - in particular, you should do it my way.”
So what is the right level of focus? Here are some heuristics I use to figure that out. They aren’t perfect rules. They aren’t absolutes. Some even pull you in opposite directions. But they can work generally.
If you prioritized your list of things that would move the business forward, and started going from top bottom to top on what’s easiest to cut, or what you feel comfortable giving up, give up one more thing past that. If you have 7 things you really want to do, and when forced into it, you can drop 3, drop 1 more and that’s probably (but not always) a good level of focus.
If you are a technology looking for an application (most VCs won’t back you but a few of us really like these, so email me), don’t focus. Go broad for a while. Sign a few pilots with different types of customers. You need 12-18 months to cast a wider net to see where you get traction.
If you are a solution to many problems, then it depends on the shape and scope of those problems. If it’s lots of little problems (maybe something like Zapier in the early days - connecting lots of things to lots of other things) I’d go broad and just build all the use cases I could build, and ignore most definitions of “focus.” If there is one or two really valuable problems you solve, start with those but, if they are difficult to get traction (maybe customers aren’t aware of the problem, or entrenched ways of thinking are too strong) then move to the secondary ones and use those as leverage to come back to the main value prop use cases.
If you go 12-18 months and don’t find a focus point, try artificially focusing for 6 months. Re-do the website to target just one buyer persona or industry vertical and gear all your messaging towards that and see if it moves the needle. It’s 50/50 in my experience whether it will work.
If you have to pick a focus and you wonder if it should be industry vertical or not, use this test - does your buyer go to a job title conference or an industry conference? If your Director of Customer Support for a Financial company is likely to go to a Support conference over a Financial conference, vertical targeting may not work well. Try buyer persona focus instead.
Remember that even as you start to focus, it won’t really become clear until around $1.5M to $2M in revenue. Before that, you will sign up customers that feel a little more mixed, even if you are consistent in your messaging. That’s ok. Don’t turn them away in the name of focus if your value from engaging them is more than the distraction factor.
Keep in mind that a startup is an organization searching for a profitable business model. You should be experimental until you find that business model. Then you should focus on exploiting it. Conventional wisdom that comes from most business leaders doesn’t apply to the “searching for a business model” phase, only the execution phase, so take it with a grain of salt.
Ultimately, what you are trying to do here is make educated guesses about the tradeoffs you have to make to build your startup. Do you think you close more deals by adding one more feature that caters to your existing target customer profile, or by adding some features that expand your TAM by catering to a new type of buyer? If it’s the latter, but your investors tell you expanding like this “not focusing” then they are wrong.
Finding the right level of focus in a business is hard. AI technology is likely to make it easier than ever to do more with less, meaning existing ideas of what is appropriately “focused” might be shifting. You can serve bigger companies earlier than you used to be able to. You can go international sooner. You can launch a second product sooner in your company’s life cycle. Maybe these things are unfocusing. Or maybe they are right for your business.
All I’m saying here is don’t take this common advice “you should focus” at face value. Make investors do the intellectual legwork to understand your business and help you find the right level of focus that is appropriate for what you are trying to do. Don’t let them get away with simple aphorisms that aren’t easily analyzed. That’s intellectually lazy and won’t lead to good outcomes.
Thanks for reading. And by all means if you have related points to make, or disagree, please let me know. I’m always open to other points of view.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/5-contrarian-ai-theses-for-early,"Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Nova, the company behind Brandguard. I also host the AI Innovators Podcast. Let me know if you have suggestions for interesting guests.
As an angel investor, I constantly shape and re-shape my theses. If you’ve followed my writing on AI (since 2016!) I’ve shifted my points of view on many things over time. This week, I want to explore 5 AI theses I’m mulling over that are less common. The goal is to prime your mind to watch for the evidence that these are true or false, so that you can shift your perspectives with the evidence.
Horizontal LLMs will lose. This thesis comes in two flavors. The first is, LLMs generally will lose when they get displaced by a new and better technology. Many of the people I know who work at the cutting edge of AI believe LLMs will not get us to AGI, and that the next leaps will come from something new, not just more data. The second version of this thesis is that LLMs will verticalize to the extent it probably makes more sense to use multiple versions of vertical LLMs than one horizontal one, for most applications. The latest wave of AI has been all about training data and compute, and if these two trends (new architectures, verticalization) make those less powerful, it has interesting implications for who wins in certain markets.
AI won’t impact full markets the way previous tech waves have - but instead will only impact specific companies. This thesis is a bit nuanced, but I will try my best to explain what I mean. Most of the time, VCs have these ideas like “cloud is going to cause XYZ changes in enterprise software” or “mobile is going to mean ABC for fintech.” They address tech changes in terms of how they impact markets, not individual companies. This thesis says that AI won’t have a consistent impact across the companies in a specific market. The way to think about AI isn’t “here’s what it will mean for this industry.” The reason this could happen is that AI fundamentally applies intelligence and learning to steps in the corporate value chain, and some companies have value chains that are much much more amenable to taking advantage of that than others. In fact, most companies try to build their workflows so you need as little intelligence and learning as possible. Best practice is to systematize everything you can. So one possibility here is that the companies that benefit most from AI may have more in common by having similar steps in their value chain rather than in terms of the product they make or customer they serve.
AI will kill most forms of competitive advantage. Some strategic thinkers have been arguing that as tech moves faster and faster, long term competitive advantages go away, and all you have is a series of short term fleeting competitive advantages. Google’s “we have no moat” memo is a good example of where this thinking currently is on AI. If intelligence and execution both eventually become commoditized from AI, where will competitive advantage accrue? What will it mean for early stage investing?
AI will bifurcate the economy into real and AI worlds. I’ve written about this before but, I’m not sure that many people agree so I’m including it as a non-popular thesis. When I speak on this topic, I always point out that at some level you run up against the laws of physics, and AI can’t change those. Concrete dries at the speed concrete dries. AI isn’t going to help us build new cities in days instead of years. Some things still take time. Investors are pouring money into industries where, at best the benefit of AI might be 20-30%, not 20-30x.
Customer acquisition channels will collapse into agents. Like many of you I’ve been following AutoGPT closely. The first order effect, if agents become the norm, is simply that we use them more for more tasks and maybe we use fewer software application interfaces. But I think not enough people are thinking about the second order effect of that which could be - the agents become a major customer acquisition channel. Why go to G2 or answer a cold sales email or click on an ad when I can just ask my agent what I should do? It may not happen because the companies building agents have a lot of issues to solve and won’t be thinking about that yet, and the companies selling you stuff have every incentive to exploit new channels, including agents. It will be interesting to see how it plays out but as an investor, you have to think about this long term. If you like a company because of their PLG or community driven GTM, it’s possible those are irrelevant in a few years.
There you have it. I’m always looking for theses that aren’t popular and evaluating them against the evidence I see in the world. These are five currently under evaluation, and they may or may not end up panning out. But in the search for success as a builder and investor at early stage companies, these are important things to think about.
Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/will-ai-kill-vertical-saas,"This newsletter is 100% written by me. No ghostwriters or GPTs.
Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Nova. We make a brand safety tool for generative AI. I’m also an active angel investor in the AI space. If you are working on something cool I’d love to take a look.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
Recently I’ve seen a lot of companies who are using AI, and often GPT-4 in particular, to write code. It’s an exciting trend, but it has me thinking… if over the next 5 years the cost to create software drops 90%, what are the repercussions?
I see three things that could happen in this future world.
Vertical SaaS becomes a bad place to invest. Part of the reason SaaS is such a great business model is that switching costs are high. The migration off a typical marketing automation system usually has a 2-3 year payback, so companies don’t move for financial reasons. But if writing the migration code, and executing on the migration takes just a couple of days with AI-based development tools, will companies switch more? Will churn rates go up?
Build vs buy decisions skew towards build. Today, it rarely makes sense to build an application internally rather than buy a SaaS tool that meets most of your needs. It takes too long and costs too much and the maintenance costs are too high. What if AI-based coding tools in the next couple of years can help you build your own CRM in days? Would you do it to get exactly what you want? What does that mean for software companies in general if building becomes cheaper than buying? Will AI tools be able to do software maintenance work as well as they do the initial build?
Non-venture backable software companies could explode. If the price of software drops dramatically, that means we will get more software. Most likely that means more fragmented software markets, and that means perhaps we see a lot of $15M revenue software companies with 5 employees and 30% net margins that can’t grow at venture scale.
Many factors beyond the price of software creation will impact this future. Customer acquisition costs still play a role, and maybe at scale those come down so much that big software companies still dominate. And these observations really only apply to workflow applications. Marketplaces and networks will still be valuable because they are more than software.
Maybe the prompt precision required to create custom software creates so much complexity and overhead - that the benefits of rapid low-cost software creation aren’t fully realized because of the requirements→prompts bottleneck.
As an early stage investor in AI companies, I never try to predict the future. I just try to understand the possible futures that could be created, and try to get upside exposure to the best ones. Or maybe in the case of the ideas above, avoid exposure to areas likely to crash.
If you’ve thought about this, I’d love to hear your feedback. Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/the-intimacy-limen-in-ai,"Happy Sunday (and Mother’s Day in the U.S.) and welcome to Investing in AI! I’m Rob May, CEO at Nova, and we make Brandguard, a brand safety tool for generative AI. I’m also a very active angel investor in the AI space, and run the AI Innovators Podcast.
I was in a meeting with some AI folks recently and heard about a study that will be released soon. I won’t go into details until then but, the gist of it is that when people are given the chance to use AI tools for many day to day work tasks, they don’t always use them, even when they can. The category of tools that most often found users skipping the AI version in favor of the real way to do it - personal relations that build intimacy. (All kinds of intimacy, not just romantic relationships).
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
As an example, say I give you a tool that automatically praises all your co-workers once a day. In this study, people given a tool that does something like that decided NOT to use it. You can imagine why. These human to human interactions evolved because they build trust. If you fake the interaction, you fake the trust. Perhaps for a one-off interaction where you don’t know if it’s a bot or not, it doesn’t matter. But for ongoing relationships where the intimacy deepens over time, people chose (at least for now, in this study) to keep the human touch.
There is a concept in biology called a limen. A limen is a threshold below which, you don’t register a stimulus, because the stimulus is too small or light. I’m starting to wonder if there is an intimacy limen in AI. In other words, if you list out all the types of interactions we humans have with each other, and rank them by intimacy, is there a threshold at which we don’t want to use AI? The intimacy limen is the threshold at which replacing the task with AI isn’t worth it, because we humans actually want to feel ourselves doing that task. AI for store checkout or drive thru ordering? Yes. AI for consoling our best friends after a breakup? No.
When you are building AI tools, or investing in them, you sometimes see an argument like this: People do X millions of Y type of action every day. If AI can automate that away we can save Z hours a year and that’s worth billions of dollars. Yet sometimes we humans really like to interact with each other. And other times we don’t but, are programmed to feel guilty if we shirk that interaction. You can’t build what humans don’t want automated away, even if it does make our lives easier. Maybe future generations that grow up with more native AI tools will feel differently, but for now, the intimacy limen is something to consider when you are evaluating opportunities for AI.
Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/why-real-user-feedback-matters-less,"This newsletter is 100% written by me. No ghostwriters or GPTs.
Happy Sunday and welcome to the Investing in AI newsletter. I’m Rob May, CEO at Nova. I’m also a very active early stage investor so, if you have an interesting AI deal, please send it my way.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
Today I want to talk about this article on the second order effects of AI, and what it means for user feedback in product development cycles. You’ve probably all seen the graph below about the increasing speed of product adoption.
I want to consider what happens to this in a world of AI tools. I’m going to argue that faster adoption, faster development, and the rise of synthetic users all together change the way user feedback will happen for AI companies.
WHAT FASTER ADOPTION CYCLES MEAN FOR AI
Change is coming faster and faster, and as we see from the graph above, the adoption of new technologies is increasing. Every new breakthrough penetrates the market faster than the one before it.
New technologies sometimes take a while to adjust to. But AI is about to bring an avalanche of adjustment opportunities as intelligence permeates every corporate and personal workflow. And many of these changes will create secondary and tertiary effects that will come from the market adaptations to the initial change. As an example, take email marketing. Initially email made it cheaper and easier to keep in touch with friends and family around the world. But it also became easier and cheaper for companies to send messages to people. That led to spam, which led to spam filters, which led to people gaming spam filters, which lead to a new equilibrium where eventually we cared less about email and moved important conversations to other channels like chat or text.
These launch and adaptation cycles were slow enough historically that you could build a business in the time window of one of them. But now those time windows are shrinking. ChatGPT launched about 5 months before this article was published, and already there are waves of impacts: students using it to write homework, tools built to check homework to see if it was ChatGPT generated, and surprisingly deep adoption of this new equilibrium by students, teachers, and schools.
It doesn’t take much math skill to see that if we shrink adoption rates from 4 months to 3 months, to 2 months, to 1 month, to 3 weeks, to 2 weeks… you get the picture. How do you spend time dealing with user feedback on a tool that will be obsolete in months? What about one that could be obsolete in weeks? If you had, say, a user interface that was changing at that rate, user feedback on any one state would be minimally useful. You can’t spend 4 months testing a user interface that won’t even be live for that long before it sees significant changes again.
WHAT FASTER DEVELOPMENT MEANS FOR AI
Let’s roll back to 2005 and the publication of “Four Steps To Epiphany.” That book launched the concepts of customer development into the startup mainstream as more and more entrepreneurs adopted those processes over the ensuing decade. But why was customer development such an effective approach?
The reason it worked so well is because building things was expensive. Following a customer development process meant exposing parts of the process to the user earlier to get feedback on whether the thing you were building was useful or not.
But what happens when AI speeds up development of everything and lowers the cost of development to a fraction of what it was previously? Sometimes I see startups that tell me things like “we interviewed 200 customers before we ever wrote a line of code.” I don’t generally like these startups because I believe you need a bias for action to be successful in this field (and this shows the opposite) but, that aside, if development is fast and cheap, it’s probably better just to build the thing and get real user feedback from the real application instead of doing a bunch of interviews before writing any code.
Customer development was trying to save startups from wasting money. But in a world where AI makes product development cheap and easy, it could be that customer development is the waste.
THE RISE OF SYNTHETIC USERS
One of the things we are learning about LLMs is that they can do a good job of capturing the average view of the world. As a result, companies like Synthetic Users are showing how to use LLMs to generate user feedback. It sounds strange but, the deeper you dig into it the more it makes sense.
In recent weeks I’ve seen companies building synthetic users of all types, for many applications. Like other types of synthetic data use cases, I think it will end up with an 80/20 split. Roughly 80% of your feedback will come from synthetic users and 20% will come from real users.
So what does this all mean when you put it together? If cycles of innovation and adoption are shortening, time to build software is shortening, and feedback can be generated largely by machines and data - how do we handle user feedback in an AI world where these things are true?
I believe the role of user feedback, the way we approach it, and it’s usefulness are all about to change significantly. Generating new ideas will be more valuable than validating the near term status quo. Breadth of experimentation will beat pre-launch customer surveys. And user feedback will be focused more on rapidly validating experiments that trying to synthesize it into a plan for what to start building.
People who work at startups love to quote dumb aphorisms like they somehow prove a point but it’s best to ignore them and remember that success if very situational. Every product and company is a little different, and while some frameworks for decision making are common across them - there’s a lot of nuance. Now that the whole thing is moving faster, your user feedback process needs to adjust to accommodate it. The companies that move their business processes to machine speed are the ones that will win.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/does-chatgpt-mean-humans-are-stochastic,"This newsletter is 100% human written by me - no AI, no ghostwriters.
Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Nova. I also run the AI Innovator’s Podcast. If you know someone who would be a great guest, let me know.
Today I want to attempt to tie together 3 concepts I’ve written about in the past and why they concern me that AI might make things worse that anticipated. The first is this idea that I wrote about 10 years ago in Venturebeat, that the internet is killing innovation. It’s based on research that shows how the internet narrowed the scope of scholarship, not expanded it.
The second is this idea that I’ve covered in various newsletters that friction can be a good thing. Tech has this obsession with lowering the friction to any action but sometimes pauses, challenges, or frictions to do something good. Those frictions, however small, mean we take it more seriously.
And the third idea I’ve written about is that AI isn’t just dangerous because it could come take over the world someday (I’m a skeptic on that). It’s dangerous because it could reveal things about humanity that we don’t want to believe because it can assimilate information at scales we can’t and (hopefully) without the cognitive biases we have. This is dangerous because we humans like to believe we are uniquely special, despite quite a lot of evidence that we are not.
Now that I’ve set the baseline with those points, I want to begin with this notion that started circulating about 2 years ago that LLMs were just “stochastic parrots.” The idea emerged as part of the debate about whether these models are intelligent, and the stochastic parrot comment was meant to explain why they aren’t very close to intelligence.
The issue I want to address is - are humans doing more stochastic parroting than we think, and will LLMs actually make it worse? Here is my thinking...
First of all, I think too many people don’t really understand what they are talking about. This may have always been the case, but I blame a lot of it on the Internet. With information being so easy (low friction) to find, it’s very easy to go 2 minutes deep on a topic whereas it wasn’t previously, and that can be misleading.
Take an issue that doesn’t come up in casual conversation very often, something like LIFO accounting. In 1992 if you mentioned LIFO accounting, and someone knew that it stood for “Last In First Out” inventory accounting, they probably also knew some other things about LIFO. Why? Because you couldn’t Google it and in 2 minutes understand the definition. Because there was friction in finding information, it was uncommon to go 2 minutes deep on anything. If you went to any level of depth it usually had to be more worthwhile because the time investment to find a resource to explain it was higher.
But knowing what LIFO stands for doesn’t mean you understand it. In the accounting world the choice of LIFO has many implications. Trained accountants understand these. For example, are there more tax benefits to LIFO if your raw materials are going up in price every year, or down in price every year? Really understanding a topic means you can manipulate it, understand how it connects to other ideas, and evaluate it situationally. I rarely see that in people, even in people deeply trained in an area - they are often just deeper stochastic parrots.
The way this plays out in the workforce is that people that graduated before the internet was a big deal are often overly impressed by partial knowledge because they assume if you know anything about a topic, it’s more than 2 minutes worth of depth. They assume this because at the time they graduated college it was hard to go just 2 minutes deep on something and know “about” it without really understanding it. So I think a lot of stochastic parrots have been promoted over the years because they seem smart about things that they don’t really understand, but really they’ve just been gaming a system that didn’t account for the post-internet ability to appear more knowledgeable than they are.
I’ve interviewed so many people who have VP or C level in their titles who, when asked about how they would run a department, they give me a stochastic parrot answer. Basically, they have seen it done a certain way and they can replicate that way. Years ago when I hired my very first VP of Sales, I interviewed a lot of stochastic parrots. I’d ask how they would sell a product like ours and the answer that came back was usually something like “well we did it this way at my previous company and that worked well.” The guy I finally hired gave me a totally different answer. He said “tell me about your customer and how they buy.” Then he proceeded to point out characteristics of the product and the buyer that would dictate the sales model we should use. He said things like “if buyers don’t know much about the space and rely on advisors, a channel model with those advisors might work best.” Or “if buyers have a lot of social pressure to buy and reference each other, splitting the sales team by geography might work best so they know a customer nearby.” He wasn’t parroting back to me a sales model with a few changes. He really understood sales and how to apply different models to different use cases.
This is an important point I want to make. Maybe stochastic parroting is more human-like than we think, and thus is indeed a big step on the spectrum to intelligence, because in my experience hiring, I’d say 75% of the workforce is doing some form of stochastic parroting instead of actual thinking.
What this means for the future scares me, because LLMs could make it worse. Back to the idea of lowering the friction to things and getting homogenized outputs - I worry LLMs will accelerate that trend. Here’s an example based on how we teach calculus.
If you ever took calculus, you know how they teach it? They still start by showing you the original definition of a derivative and show you how to work through it.
Then you learn the power rule and you think why the hell did I have to learn that stupid more complicated rule first when the power rule is so easy? It’s because math teachers don’t just want you to know how to calculate a derivative, they want you to understand what a derivative is. This is an important distinction.
Calculus is almost 400 years old, but they still teach it to you starting with the foundation of what it means. They use the slower, higher friction way, because if they don’t, you may not really understand it at the right level of depth, even though you could solve some easy derviatives with the power rule, but can’t really apply it to situations where you don’t have an example problem.
Now back to LLMs, and in particular ChatGPT. This is dangerous. It’s dangerous because it could make everyone seem, on the surface, a little smarter than they are.
I’ve been a big fan of the thought process first described in the book Prediction Machines, that the economic complement to prediction is judgment, and when the price of prediction drops, the value of judgment goes up. What does that mean in a world where becoming a stochastic parrot is easier and easier? How you can you have judgment in an area if you haven’t done the hard work to really understand it? If we all rely on AI more and more, do we lose some of our understanding, and will our judgment be worse?
If you’ve read the book Anti-Fragile by Nassim Taleb, I’d tie all this back to one of the ideas in that book - that small failures in systems help keep it robust against larger failures. When AI takes all the small failure modes out of various decision making because we can trust the AI with those minor things, could the outcome be that we are even more stochastic parrots ourselves, our judgment is worse, and the risks of a bigger blow up of some kind are building under the hood? As we start to rely on AI to make more recommendations for work, politics, and life, and these recommendations save us from minor troubles, are we setting ourselves up to be unprepared for bigger troubles - for the novel things not in the training data set of the AI - because we aren’t prepared and our judgment is weakened?
In summary, I think the internet created more stochastic parrots by homogenizing thought and allowing perceived depth in a topic without really understanding it. As a result, ChatGPT might be saying more about humans than we realize. We want to believe we are special and ChatGPT’s stochastic parroting isn’t intelligence, but I’d argue it’s the way a lot of humans actually operate. And I think that it could possibly get worse, if we rely too much on these models to make decisions for us and don’t still dig in and spend the time to really learn and understand. More areas of knowledge (business in particular) need to be taught like Calculus is - understanding it from the core. Hiring processes need to penalize stochastic parrots. And we need to find ways to continue to build judgment in humans so that we can use AI in the right ways, and not let it make us dumber.
Thanks for reading."
https://investinginai.substack.com/p/ai-and-the-great-bifurcation-of-2024,"This newsletter is 100% human, written by me. No ghostwriters or AIs.
Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Nova. I also run the AI Innovators Podcast, and the most recent episode is about using synthetic users for product feedback. Definitely worth listening. I’m also a very active angel investor (most recently in Consensus) so please reach out if you have an interesting startup.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
Today I want to write about what I believe will be the great bifurcation of late 2024 - the splitting of the economy into things that can advance with AI, and things that can’t.
The world has been awash in predictions about AI since GPT-4 was announced with results that have surprised and impressed many people. Microsoft published a controversial research paper claiming GPT-4 has early indications of AGI. Over 1,000 AI experts signed a letter asking that the major AI research labs pause, so we can have time for AI safety and security to catch up to the power of things we are building.
But I want to take a step back and think through the implications of GPT-4 (and the next few versions of similar models). Yes, things like creating digital content, writing code, building apps, and making predictions is going to get much much easier with these models as they approach human level generalized intelligence. However, that doesn’t necessarily mean they can act in the world. And when they do, through robots or some other means, they are still subject to the constraints of the physical world.
I believe we are making a dangerous mistake by extrapolating the speed of progress AI can have in a digital world to the speed of progress AI can have on the world in general. Think about this - even if you have the world’s smartest AI, 10x better than a human, there are still things that can’t get that much more productive.
In manufacturing, you can only turn a screw so fast. A robot can’t build a car in seconds. It takes time for motors to get moving, for electricity to flow, for tension to be applied, for welds to take hold. Chemical processes take certain amounts of time to work. Concrete takes time to dry. AI may be able to help improve some of these processes but, it can’t instantaneously dig an oil well or frame a house. It runs up against limits defined by the laws of physics.
I think we in the tech industry tend to be overly optimistic about the breadth of the impact tech can have. Reading books like, How The World Really Works by Vaclav Smil can help to reset that - pointing out how much of the workflows of the world still rely on cement, fertilizer, and oil. These are massive industries that are governed at some level by physics and chemistry and things AI can’t change 10x.
What I think this leads to, over time, is a bifurcated economy. We saw a bit of this over the last 20 years where things computer and internet related saw serious deflation while real world goods saw inflation instead. More of the economic gains also accrued to industries with more intangible assets like tech and finance.
I expect that trend to accelerate starting in late 2024, which is why I think we will look back on 2024 as the beginning of the great bifurcation. This will be the time when new AI models (GPT-5?) really start to have massive impacts on everything non-physical, exacerbating the more minor bifurcation between intangible and tangible assets we’ve seen in recent decades.
What impacts does this have on society? What does it mean for investment in startups? How does it impact business models? There are many interesting questions that will need to be answered. But rather than think of AI as a tidal wave that washes over the whole world, improving everything, we should think of it as a rapidly flowing group of tributaries that deeply nourish some places while only having a modest impact on others. It’s important those of us grapple with the possible sociological implications of this coming bifurcation, and design a system that works for the broader benefit of society.
If you’ve thought about this, I’d love to hear your feedback as always. Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/how-ai-will-make-the-next-banking,"Happy Tuesday and welcome to Investing in AI. I’m Rob May, CEO of Nova and active angel investor in AI startups (most recently FeatureForm and MuseTax). I also run the AI Innovator’s Podcast. Reach out if you’d like to be a guest. I had a post geared up for Sunday, which I’ll now publish later, because I was watching the whole SVB debacle over the weekend, and it make me think about the topic I want to discuss today.
Several people wrote about how digital banking made the SVB run possible in ways that wouldn’t have been before. I believe the stats I read were that when IndyMac failed during the financial crisis, the bank run withdrew $10B in 16 days. With SVB, it was $41B in 48 hours. That’s almost 25% of their deposit base. I’m not sure any bank in the world could withstand that.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
In parallel, there was some online chatter about tools that can fix this in the future. I’m sure we will see more services that automatically split deposits among many banks, and other creative ideas. But I want to approach this from the AI angle. What if one of the tools that emerges is an AI agent that monitors bank health? By looking at online chatter, financials, and understanding the bank’s investment portfolio, it can give you a score on bank strength, and also predict the odds of a bank run.
Sounds like a great tool for your company, right? Yet think through the implications if we all use that tool.
As an investor in over 100 companies, I saw a lot of email traffic. Some founders were sending emails about how, they were moving from SVB to First Republic or Mercury or some other bank. Other founders who didn’t bank with SVB were sending notes that they were banking with First Republic or Mercury but heard they will be next to fail, so they were moving somewhere else. So some of the companies leaving SVB were just moving to banks that were about to experience their own run.
The point is - in a world where all of this is automated and accelerated, these AI agents could inadvertently trigger a run on one bank, which could lead to runs on others, which could cause a lot of instability in the system. AI looking out for our individual corporate needs could collectively cause a bunch of problems.
The best way to fix this is friction. Tech is OBSESSED with lowering the friction to everything and I don’t believe that’s always a good idea. It was a good thing that SVB was closed over the weekend. It gave regulators a chance to pause and work through options. Introducing more friction back into the system is probably the best solution, but it won’t happen. There is no way. It just isn’t the way we think about the world anymore.
The only other solution that I see is that we need some sort of collective action AI models for these situations. I don’t know how that would work. It would have to be an AI version of Rousseau’s “social contract.” Rousseau starts with the idea that we all have rights that we voluntarily give up in order to impose duties on the others. I think Rousseau would say that it’s entirely my right to stab you in the neck, and it’s entirely your right to do that back to me. But instead of running around the world in fear of constantly being stabbed, we all agree to give up our rights to stab each other and to punish stabbers, so that we might have a better society that is more peaceful and functions much better.
What’s the right AI oversight system to put in place that looks like a social contract? What will make sure all our individual AI agents don’t act so fast that they trigger a cascade of devastation when calm, patience, and a little more friction would have helped keep things stable? Who should own, train, and run these models? It’s not something I’ve thought about before so, if you have thoughts, or are working on this, I’d love to hear from you.
Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/the-ethics-of-persuasive-ai-chatbots,"Note: Not a single word of this newsletter was written by AI
Happy Sunday and welcome to Investing in AI. I’m Rob May, CEO at Nova. In the past I’ve written about how chatbots could be problematic for society as their quality improves. Someday every chatbot will be as good as the best salesperson at understanding your behavior and responding to get you to buy. What does that mean for sales, and what is the line between helpfulness and manipulation?
Today though, I want to look at chatbots through the lens of dating. I recently had a conversation with an AI ethicist who has studied this type of issue, and it struck me as a complicated one worth highlighting.
Imagine in the future that before Joe has a date with Susan, he can take a chatbot built off all the videos of Susan, things she has written, etc, and practice his conversation skills on the bot. This will make his first date go much better. If Joe is shy, introverted, or has poor social skills, this can be a very helpful aid to his dating life. But what if Joe is manipulative? What if he doesn’t really like Susan but wants Susan to like him, and now he’s figured out how to do it to achieve his ulterior motive, whatever it may be?
I don’t know if it still exists but at one point, Harvard offered a class on optimizing your online dating profile. A friend of mine who took it gave me some hints for men. For example, always put a picture of yourself with other people so you look social. Say “my friends say I’m…” rather than describing yourself in first person. There were key themes a guy was supposed to mention that women really like to hear. I tried them out on my own profile. While it helped me get more matches, I ended up going on dates with people who actually wanted someone more like my optimized profile than with the person I really am. That wasn’t very helpful.
As a result of that experience, I question the value of these types of NLP tools that will help people smooth over certain types of conversations. They are definitely needed and can be helpful in many areas. But they need to be done in a way that retains authenticity. In fact, a lot of the downside of using more AI could be a lack of authenticity in many areas of life.
Think about what the Internet did to media? It spoon fed us mass market linkbait way more than it highlighted the long tail of great and unusual stuff. I think this could be a model for what happens in an AI world as well. AI tools that teach people how to communicate more effectively at work, on dates, in relationships, with children, and in public, could do the same thing. They could collapse the diversity of conversation around the most generically popular things to say. Both sides of the bell curve of communication skills in society could be pushed closer to the middle as expectations shift because of what we get used to with AI.
You could say technology is neutral and how you use it is up to you. Or you could say that technology is never quite neutral but does lend itself towards some use cases more than others, naturally. For all the concern (rightly so) about problems in AI like bias and fairness and explainability, I think the societal impacts of the type I’ve described here aren’t given enough consideration.
AI can change our behavior in good and bad ways. We have to be thoughtful about the type of society we want as we build these tools. Whether something is “right” or “wrong” will often depend on what we are trying to accomplish. If you are working in this area, I’d love to hear about it.
Thanks for reading."
https://investinginai.substack.com/p/the-economics-of-foundation-models,"Hi Everyone and welcome to Investing In AI. I’m Rob May, CEO of Nova, and a very active angel investor in the AI space. I also host the AI Innovators Podcast. This newsletter is 100% written by me. No LLMs involved.
I’ve been writing about foundation models and the possible ways the future could play out for them, and now I want to look at the pro scenario and the con scenario for these businesses.
Foundation models have been credited with supercharging AI recently. And they are creating a paradigm shift in AI. One good mental model for thinking about foundation models comes from Percy Liang:
The term ‘foundation models’, he explains, is meant to evoke the central importance of these systems within whole ecosystems of software applications. “A foundation of a house is not a house, but it’s a very important part of the house,” says Liang. “It’s a part of the house that allows you to build all sorts of other things on top of it. And, likewise, I think a foundation model is attempting to serve a similar role.”
The question I want to address today is - would you want to be in the foundation model business? Before I give you my thoughts on situations where you would not, and situations where you would, I do want to clarify that there is a lot of nuance in these decisions. Some areas of foundation models will inevitably more competitive than others, and thus possibly less attractive, and some markets for certain foundation models will be smaller than others.
Let’s start by making the “no” case. In this case, the foundation models get widely adopted and that growth makes investors excited, but the underlying economics remain poor and VCs pour billions into subsidizing a business model that never emerges. This is what some people claim happened with companies like Uber and Doordash, which continue to lose money as publicly traded companies.
There are several ways we could end up in this situation. First of all, the marginal cost to gather more data to train bigger models could rise, while the marginal value of a better model falls. This is the reverse of what you want if you run a foundation model company. As your economics get worse, and new types of computer chips dramatically lower the cost to train these models, the defensibility of the whole business model goes down. On top of it, the scale of customer usage bogs down the company with more traditional engineering challenges that steal resources from AI innovation that could be much more valuable. In this scenario these companies may run losses indefinitely and you don’t want to be in this business.
Now let’s consider the “yes” scenario. In this situation, as adoption climbs the fixed costs of training and serving a model get spread out over more users and lead to eventual profitability. Competition stays tight with just a few players dominating the landscape. Advances that come from better chips or other widely available academic innovations are adopted fasted by the main players and they keep lowering their costs enough that rolling your own foundation models doesn’t seem worth the hassle.
What I think is happening is that the market, while realizing the possibility of the “no” scenario but desiring to get to the “yes” scenario, is starting to differentiate in different ways. Consider OpenAI and Cohere as examples. I know people at both companies but don’t have any inside knowledge of their long term strategies, but as an outside observer, here is what it appears they are each doing with respect to LLMs.
It appears the market may play out like the cloud computing market, with trained models and the tools needed to run and implement them sort of like various levels of compute abstraction - you can buy a bare metal server or a virtual machine, with levels in between. I think part of the OpenAI-Microsoft deal was for OpenAI to get access to other training data to push models forward. It’s possible that Microsoft will slide some term into Office licenses that allows it to use data for training purposes, or will find a way to make it fit their existing terms. This would continue to push OpenAI forward as the leader of horizontal AI use cases for absolute best of breed models, and they will probably quickly discount slightly less performant versions for other use cases. In this case, OpenAI becomes profitable because advances in model capabilities continue to unlock more TAM via more use cases.
Cohere, on the other hand, seems to be building lots of enterprise functionality that makes those tasks easier that are one step above being a pure foundation model. They can be, probably 98% as good as OpenAI or other models and win customers all day long with this approach, because if you are building common NLP tasks into your app, Cohere may end up being the enterprise choice. In this scenario the company becomes profitable because you are paying primarily for that extra layer above the foundation model.
The other issue that can come into play here is that maybe none of this is where foundation model companies actually make money. Perhaps by staying at the front of the pack, the distance between the market leaders and the capabilities of everyone else become bigger and bigger, and the true business model emerges from use cases that aren’t coming for another couple of years.
I think what will happen is similar to what happened in the cloud computing world. Compute costs for both training and inference on a per-unit basis will drop dramatically in the next few years, the way storage and server costs dropped for the cloud. Value added service layers on top of the basics will create demand and stickiness, the way they did for the cloud. Competition will shrink over time as the clear market winners emerge. And these business will ultimately be a good place to have bet.
I also suspect the market will be large enough to support several niche foundation models which can still have hundreds of millions in revenue. If you have ideas for those and are early in your fundraising, I’d love to hear from you.
Thanks for reading.
Last Week in AI
The AI Scaling Hypothesis
The past decade of progress in AI can largely be summed up by one word: scale. The era of deep learning that started around 2010 has witnessed a continued increase in the size of state of the art models. This has only accelerated over the past several years, leading many to believe in the “AI Scaling Hypothesis”: the idea that more computational resourc…
Read more
2 years ago · 6 likes · 3 comments · daniel bashir and Andrey Kurenkov"
https://investinginai.substack.com/p/the-forces-driving-the-economics,"Hi Everyone,
We’ve had a lot of new signups recently so for those who don’t know me, I’m Rob May, CEO at Nova, and active angel investor in the AI space. I also run the AI Innovator’s Podcast (formerly called the Investing in AI podcast) This newsletter is primarily to help me think through what is happening in the AI space. I find that writing helps me clarify my thoughts, and I hope you can benefit from that.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
Today I want to talk about Generative AI and the forces driving the underlying economics of it all. I wrote previously about why I think it’s mostly a bad bet, but I still think there are pockets that will be really valuable. And I also think what happens with generative AI will impact other pieces of the market and provide other opportunities for economic complements and other tangential technologies to shine.
What is both exciting and nerve-wracking about investing in this space is the chaos. There are many trends that are shaping how this will end up when it reaches a stable equilibrium as an ecosystem, and weighing the factors and how they might pan out is difficult. I believe that making bets in markets like this, while many people sit out and wait, can lead to great outcomes if you are correct, and that thus it is worth the risk.
What follows is a list of random thoughts on forces shaping the eventual outcome of generative AI, particularly as it applies to media.
Legislation - At the moment, I think politicians are not concerned enough about the possible impacts of generative AI. I hear some speak on occasion about the risks of deep fakes and fake news but they don’t grasp the full scale of what could come., As the problems become more concrete as these technologies spread, I expect legislation in the next few years. What form that will take is hard to predict because few politicians have a good deep understanding of the potential and risks of this technology. To me, this is one of the highest areas of uncertainty.
Better AI computing chips - It currently costs millions, often tens of millions, to train these massive generative AI models. But all kinds of AI chips are coming that promise to lower the cost dramatically. (I’m personally an investor in Rain and Mythic but there are dozens of interesting competitors and approaches). If a new chip takes a model that cost $20M to train on GPUs and drops the cost to $200K, that suddenly makes training your own model available to most businesses, provided your use case doesn’t require constant re-training. Inference can also be expensive for some use cases, and these chips promise to lower the price of that as well. If it breaks the perceived oligopoly in foundation models, what does that mean for those business, and for others? Maybe the oligopoly will continue to exist, and hold for reasons other than compute costs for model training.
Foundation model oligopoly - At the moment, foundation models are expensive to train and therefore limited to big tech companies and really well funded startups. If this oligopoly holds, and there are just 5 or 6 companies that train and run large generative AI models at scale, is that enough competition to hold down prices for customers? Do they differentiate enough on use cases to give themselves high margins and charge customers high rates? Do they eat into the upstream and more vertically targeted use cases to improve their own margins, or not? And if they do, does the FTC (back to point 1) step in? If models become cheap to train does the oligopoly still hold because of historic path dependent reasons, or maybe because of branding around output quality and certainty of certain levels of service or quality? These are all important issues to watch as this plays out.
Limitations on further training data - I’ve heard that GPT-4 was trained on so much public text data that OpenAI isn’t sure where to go next to get an order of magnitude more. Whether that is true or not, it definitely will become an issue at some point. Will it matter for the economics of generative AI? Maybe the OpenAI-Microsoft partnership will help OpenAI get access to more private data (corporate Word docs?) that help continually push the training data set to a larger scale. But at some point it starts getting hard to find more data. Maybe the models will be so good that we don’t care and pushing them forward isn’t an imperative. Maybe we have humans label certain data types at mass scale to help the models, in which case the time to do so could be the bottleneck to the next level of breakthrough.
Small data AI algorithms - There are many ways to do AI other than training large neural networks on massive data sets. Fewer people are working on this area but it does have believers. And there are people constantly looking for new algorithms for intelligence. One of these could come out of left field and make it easy to train and run LLMs and image diffusion models on your phone. It’s doubtful that’s around the corner but, it’s a possibility.
Social confusion about machine vs human content - Regardless of what happens at the technical level, if an explosion of content of all types confuses and irritates humans to the point that it becomes major problem, something will emerge to help. That could be something else in this list (Legislation, New Tech, etc) or it could just be changes in social behavior, like assuming by default things were machine generated. That could lower the economic value of content of all types, which will shape the eventual market equilibrium.
Poisoning the well problem - This is related to the point above but, in general, there could be so much content created that for the content types supported by generative AI, people just tune out because there is too much junk. In this case, nothing new emerges to solve the problem we just deal with the content wasteland.
Declining marginal value of a better model - One thing we already see is that, when a new model comes out, various smaller, cheaper, and sometimes open source versions of it quickly proliferate. These versions often have limitations compared to the original but are appropriate for certain use cases that don’t need the full blow version. By the time GPT-5 comes out, the market perception could very well be “awesome, but, it’s really not worth it as GPT-4 is good enough for 98% of our use cases.” I don’t know if or when that will happen. But as an investor, it’s something to consider.
Unexpected benefits from generative AI use cases - With the success of Stable Diffusion, people are asking where else we can apply diffusion models. There is no doubt that some applications will be surprising, and that some existing technologies will be improved indirectly from these generative AI advances. Predicting when and where is difficult, and requires experimentation.
New layers of the tech stack emerging - When you look at some of the issues caused by generative AI, particularly numbers 1, 3, 6, and 7 in this list, you can see opportunities for new products and services that emerge to help you navigate generative AI. When the Internet came along and content exploded online, Search became valuable. What’s the equivalent thing that will happen in a generative AI world? (At Nova, we’ve been working on an orchestration and automation layer that helps solve some of these issues, so this is at the forefront of my mind.)
With all this chaos, what’s an investor to do? First of all, make bets. You want exposure to this type of uncertainty because the upside in these areas if you bet correctly could be massive. Secondly, keep your bets smaller than normal when the uncertainty is high. This is a time to reserve more capital for your winners once it is clear who they are (if you do follow on) or make more investments (if you don’t do follow on). And finally, keep a close eye on how these forces play out, and when you see something that might be a tipping point, something that will dramatically increase the likelihood of a specific outcome in the future, bet big.
This is a really exciting time for AI, and how the economics of these new technologies align, where value is created, and who benefits the most in the long run are all still open questions in my mind. If you are have an opinion, something to add, or a different point of view, I’d love to hear it.
Thanks for reading.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/why-generative-ai-is-mostly-a-bad,"People often ask me what I think about generative AI. It’s awesome, and terrible, depending on your point of view. From a technical perspective it’s a huge breakthrough that will have lasting impacts. The things we can now do with tools like ChatGPT and Stable Diffusion are pretty remarkable. But technology revolutions often impact shake up the economics of various industries and business models, and by that measure, generative media is a doozy. As someone who has been writing about AI, building AI startups, and investing in AI for the past 8 years, I’ve had a lot of investors reach out to discuss these changes, and I thin the vast majority of them are thinking about the economics of generative media the wrong way.
I think generative AI, the way most investors are currently playing the market, is a bad bet. Let me explain why, and where I think the real value lies.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
My perspective on investing is that long term value creation is driven by growth, innovation, and defensibility. That last point is the big one - defensibility. As technology makes doing more stuff faster and easier, it’s increasingly difficult to find areas of long-term defensibility in business models. In particular I don’t see much in generative AI. The platform side - the creation and maintenance of these models has some defensibility in the near term, the next few years, but most people can’t play there. That market will be an oligopoly. It takes tens of millions of dollars to collect data sets and train these models and right now that is only available to the large tech companies. In the long run, new chips may change that equation but, that’s still a few years away.
The key position investors seem to be taking is that “context layers” that take these generative tools and put them into some point solution of a workflow is the place to make a bet. They believe the big tech companies will own the foundation models and so, contextual layers on top of those models that enable workflows like copywriting or image editing or whatever is the way to go. Dozens of generative media point solutions are getting funded.
My opinion is that these business models are not defensible for two reasons. First, there will be too many players because the barriers to entry are low and that drives a competitive dynamic that is unfavorable to investors. Second, they risk competition with the foundation models themselves as those models improve.
So where do you play in this market? My framework for thinking through this is simple. When the cost of something trends towards zero because of new technology:
You will get an explosion of that good.
That good will decline in value and defensibility
The economic complements to that good that see increased demand as a result of the explosion in the original good, will be the place to invest.
To give an example, think about printed news content before the web. You had to have a printing press and a distribution network for your paper. These were expensive to build and maintain and thus local papers had little competition. Then came the web. Now anyone could start an online publication! And for the next two decades, there were very few success stories because that market was too competitive. Margins for news organizations dropped, and were difficult to recover.
When there was suddenly an explosion in content from the web, where was the bottleneck? Search. Finding good stuff in a sea of content became the new economic choke point, and Google became worth more than all of the other newspaper companies combined. The new media organizations that started because it was easy and cheap to start one, that maybe were vertically targeted or, the equivalent of our “context layer” to generative media, made a little profit here and there but, by and large it went elsewhere. I think generative media startups will see a similar trend.
If that is true, where does the value accrue as a result of the generative media explosion? When it becomes really easy to create longer form content and imagery, I believe there are two things that go up in value. The first in orchestration. Coordination layers that help humans deal with the deluge of creative assets they will soon have to navigate will go up in value because doing it well will be difficult.
The second is trust and protection of things like Brand. When machines allow your team, or anyone on the internet, to easily create tens of thousands of images related to your brand, how do you monitor and protect that? How do you insure brand consistency in your own workflows? How do you spot imposters and fakes when they are easier than every to create?
These will be difficult problems to solve, but they will be profitable ones. I’m convinced enough of this that I’ve launched Nova, which has been over a year in development working on exactly these issues. If you run a marketing team and are interested in using generative media, we’d love to chat.
Thanks for reading, and as always I’d love any feedback you have if you disagree with any of these points.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/will-ais-have-free-speech,"Hi Everyone, and welcome to my new home for writing. I’ve written some posts here over the last year, but as I phase out of InsideAI, this will be my primary outlet. The focus of this substack is Investing In AI but, I use the term “investing” broadly. While the focus will be on financial investing in AI companies, it’s also important to cover investing time and effort and attention and other resources, and so there will be posts covering those things as well.
Watching the Twitter drama, in particular the debate over whether or not Trump should be allowed on Twitter, raises a lot of interesting questions about free speech. Today I want to ask the question - should an AI have free speech? What if it’s dangerous? What if it’s the world’s most persuasive AI, able to personalize arguments at scale to be convincing to many types of people, maybe even most people.
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe
In 2014 I remember visiting one of the customers of my startup at the time. We sold into I.T. departments and this was the head of I.T. at a major growth stage silicon valley company. He had spent his life in political technology and so I asked him why he moved on.
He told me he had worked on many campaigns, but his last one was the Obama 2012 campaign and, at this point the technology had gotten so good that (I’m paraphrasing here from memory) “it could tell you how to change your message street by street, house by house, to tell people exactly the thing that would resonate most with them.” He quit because he felt politics shouldn’t be that way.
Someday someone will compile a good data set of arguments, and use it to build a chatbot that tries to convince people of various points of view. That will lead to a bigger data set and that will lead eventually to a very convincing chatbot on a host of topics. Let’s say one political party national committee, or one consumer company, or one entity that has an agenda other than the broad welfare of humanity owns this bot. Would you allow it to have “free speech” and be open on the web, on Twitter, or wherever else it wants to be?
We often think of free speech as something for humans in democracies. What about bots as they get closer to human status? What was the original purpose of free speech, and does a new AI use case still fit? These are complicated questions without easy answers. For now, I just want to stimulate discussion and general thinking about these issues, but hope to address them here in coming months in more detail, and possibly a framework for thinking about them.
Thanks for reading.
Rob
Thanks for reading Investing In AI! Subscribe for free to receive new posts and support my work.
Subscribe"
https://investinginai.substack.com/p/the-mental-model-most-ai-investors,"I haven’t written in a while. What prompted me to start back up though (other than finally getting to a spot where I had time) was my frustration with the lack of a good mental model for technical reflexivity in most investors - particularly in AI.
The idea behind reflexivity has been around forever. You could call it a form of a feedback loop, or maybe a piece of cybernetics, but George Soros was the first person to give it a name and point out it’s relevance to investing. The premise is this:
Reflexivity in economics is the theory that a feedback loop exists in which investors' perceptions affect economic fundamentals, which in turn changes investor perception.
Easy enough to understand. But now let us add a level. There is a feedback loop between some types of new technologies and the ecosystem that technology is in, or creates. This isn’t a situation of someone building a faster processor and so more people want it and then it grows fast by getting built into more systems. That analysis is linear.
This is about a process of creativity that stems from changing the status quo and realizing what is possible. People get used to things. People think about the world through the lenses provided by the status quo of the things they use. Then when the world changes, sometimes whole new ideas are possible. The strongest example of this is probably the Web. By connecting computers together, it enabled all kinds of ideas that people didn’t think of before. The network of interconnected computers provided a new mental model for them to work from to invent new things.
Social media didn’t immediately come with the web. Why not? My theory is that it takes time for the new reflexive part of an innovation to arrive. To understand what is fully possible under the new technology paradigm, some people need to have worked in it natively for a few years so that they begin to break down the status quo way of thinking.
The best example I have of this is investing in AI computer chips. Two of my best investments have been Mythic, an analog crossbar array chip, and Rain, a neuromorphic chip. In 2016 when I started looking at AI chip investments, most investors I showed them too said “NVIDIA has this solved, you can’t compete” which was entirely wrong.
Later, when AI chips started to get traction, the investment reasoning was, in my opinion, still too shallow and linear. Most people thought that there will be X number of AI workloads and these chips will address the TAM of those workloads. But they missed the reflexive part.
When developers spend a couple of years programming on a chip like Mythic or Rain or one of the many other non Von Neumann architectures that are rising up in popularity, they will being to think differently about what is possible. The reflexivity arises because engineers won’t simply port workloads to these chips - these chips will in turn influence how they think about possible workloads. That will lead to new ideas, and I suspect a new ecosystem of innovation centered around what is possible with new chip architectures. I don’t know what those innovations will be, but I can predict that they will exist in some form.
Bringing this all back around to my initial point on reflexivity - the reason I’m so excited about the AI chip market is not the linear TAM of mapping existing AI workloads to these chips - it’s the reflexivity TAM of what is possible when these chips instigate a whole new paradigm of what is possible computationally. That’s the mental model to use to invest in a market like this.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/data-traction-how-to-evaluate-ai-f30,"Happy Sunday and welcome to Investing in AI. I’m Rob May, a Partner at PJC, where I focus on seed stage AI and robotics investments. I’m also a participant in the ScalingAI AngelList syndicate for growth AI companies, which you can join if you want to make some growth AI investments.
Be sure to check out our Investing in AI podcast as well. This past week I interviewed Rob Aitken from ARM about the cambrian explosion in AI hardware.
— Interesting Links —
How GPT-3 And AI Will Destory the Internet. ReadWrite.
The Business Value of Clustering Algorithms. Venturebeat.
Elon Musk Has No Idea What He Is Doing With Tesla Bot. IEEE Spectrum.
OpenAI Codex Demo - Live. Youtube.
Next-Gen Insurers Are Going To Need Way More AI Horsepower. Nextplatform.
— Commentary —
If you missed part one of this post series, you can find it here.
AI companies take more time to build than the SaaS companies we have become so used to. When the normal metrics like Sales, Churn, CAC, aren’t there yet, or are bad numbers, what can you look at to figure out if a company is heading in the right direction?
When I look at pre-public-launch AI startups that only have beta customers, or sometimes no customers, I tend to focus mostly on data and model related metrics. If I can see a model getting better faster, at a reasonable cost, with some emerging defensibility, it might make sense to invest ahead of the traditional metrics and take the risk that those metrics materialize or don’t. Some of the metrics I evaluate are below.
Marginal Performance Value of Additional Data – The amount that some standard chunk of data (1 new customer, 1K new images, etc) improves a ML model, on average. I’ve seen companies with 30% accurate models and human-in-the-loop get to 95% accuracy models over a couple of years.
Automation Rate – What percentage of a human task is automated by a model? If it’s growing, that’s a good thing. Competitors will need time to catch up.
Human-in-the-loop rate – What percentage of model outputs need to be touched by a human? If it’s declining, that means the margins will improve.
Data Drift Rate – How long (if at all) does it take for the data set to drift so far it’s irrelevant? This is usually difficult to measure exactly, but you can get a feel for it.
Model/Market Fit – How well does the model perform for some core group of customers? If it fits well, they will eventually pay.
Workflow Modification Rate – This is how much of the workflow has to be modified for the customer to adopt a ML solution. You can think of it as internal operational friction to adoption. The lower it is, the faster the adoption will happen. The higher it is, the more defensible and category-creating the company could be. So this has to be evaluated in the context of other variables.
Model Performance Rate – How well does the model perform on real world data sets? If there are public data sets to use it on, this could be great for marketing.
Data Augmentation Expense – What cleaning, annotation, and augmentation has to be done to data to get it to a format ready to train a model? Some expense here is a good investment in defensibility if it’s hard to do. But too much expense means you may run out of cash before you build a business.
The economics of how you get data, build models, and how those models perform against customer needs are key factors in building AI-first companies. Evaluating them like SaaS businesses is just wrong. For many AI focused investors, this provides us a chance to invest in great businesses that others pass on because they don’t understand. But there is risk as many of these metrics aren’t well understood by either investors or entrepreneurs. The best practices are emerging, but will still take time to solidify into a new paradigm of how to evaluate AI companies.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/data-traction-how-to-evaluate-ai,"Happy Sunday and welcome to Investing in AI. I’m Rob May, a Partner at PJC, where I focus on seed stage AI and robotics investments. I’m also a participant in the ScalingAI AngelList syndicate for growth AI companies, which you can join if you want to make some growth AI investments.
Be sure to check out our Investing in AI podcast as well.
— Interesting Links —
The AI Wolf That Preferred Suicide. OneZero.
Training AI To Think With Analogies. Quanta Magazine.
The World’s First Patent That Lists AI as an Inventor. GlobalPost.
China’s Sputnik Moment. Foreign Affairs.
A Reflection on How AI Will Change Conflict. Economist.
The Future of Deep Learning Is Photonic. IEEE.
— Research Papers —
Guided Disentanglement in Generative Networks. Link.
Generative Adversarial Networks in Time Series. Link.
— Commentary —
There was a time when software was a pain in the ass to buy. You had to go through your I.T. department, no matter how small or non-critical the software was to the organization. You needed your own server to install it on. You had to negotiate a contract, and a maintenance package. Sometimes you needed the help of the company that wrote the software to get up and running. It was slow. There was a lot of friction. If you are over 35, you may remember the tail end of that world.
SaaS came along and changed all of that. There are no different versions of the software, it’s just a subscription to one giant code base running in the cloud. All you need to sign up is a credit card and a web browser. It was, and is, very low friction.
Transition points like this, Retail -> Ecommerce, Packaged Software -> SaaS, present challenges for investors. New metrics come along like “eyeballs” instead of revenue, or “MRR” instead of bookings. Initially people try out metrics, some work, some don’t. Even the ones that stick are often misused or poorly understood. But using the new metrics is much better than attempting to make sense of a new company with the old ones.
Now AI comes along, and we are seeing this issue again. Investors look at AI companies through the lens of SaaS expectations, and you know what? By those standards most AI companies SUCK.
I’ll give you an example. There are many companies in the marketplace now trying to auto-create content for you. When you initially sign up, the first few things they create are terrible. It’s not the fault of the AI - the issue is they haven’t been trained yet. AI tools need feedback from you. Many of these companies, as a result, have very high churn rates compared to SaaS companies.
As an investor, you would look at these metrics and, based on your experience with SaaS, you would say “there is no business here.” Because in SaaS, if you have a 12% monthly churn rate early in the company, it doesn’t matter if it gets better. It’s so so so far off you wouldn’t even consider investing in this SaaS platform. But with AI companies, the way to think about it isn’t as 12% monthly churn. “Churn” here doesn’t have the same sources as “churn” in traditional SaaS companies.
For an AI company, some churn might be to traditional SaaS company issues - low product value, bad user interface, better competitors, poor product-market fit. But in AI companies, while those same things may be an issue, we can add other issues. The three I most commonly see are: 1) expectations for how and why the product is used are off - this happens a lot in new markets when people are figuring out exactly how a new paradigm should work, and 2) initial performance is weak - this happens a lot because you haven’t trained the model yet, so it isn’t very customized to your needs (which is the core value prop sometimes), which leads to 3) friction because it takes time to train the model and we no longer have the patience to adopt any software with even the smallest bit of friction.
Your job, as an AI investor, is to figure out the sources of churn. So what do you look for? If some people stick with the product, and those that do consistently perform well, then it’s probably a good company with more AI related churn issues that will go away as the market evolves than broader churn issues that will stay.
The lesson here is that to evaluate AI companies, you need to look beyond the metrics we are used to. Some of them are still relevant, some of them aren’t, but most importantly - new metrics matter. What are these metrics and how, as an AI investor, should you think about them?
Much of this is still being worked out, but in part 2 (published in two weeks), I’ll take a stab at some that I look at, and what I think is important to understand.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/investing-in-robotics-what-drives,"Happy Sunday and welcome to the Investing in AI newsletter. Also be sure to check out our podcast. The last two episodes are both great - Dave Girouard from Upstart and Karim Lakhani from Harvard Business School.
If you want to join an angel list syndicate focused on investing in AI, I suggest ScalingAI, which I’m a member of.
— Interesting Articles —
Microsoft Uses GPT3 To Let You Code In Natural Language. TechCrunch.
The Ghost Work Behind Artificial Intelligence. Slate.
One Study Finds Business Leaders Are Disinterested in AI. ZDnet.
TinyML: What Is It? Dataconomy.
How The Financial Industry Can Apply AI Responsibly. IEEE Spectrum.
Voice AI Politeness. Voicebot.
Startups Using AI to Tackle Climate Change. Forbes.
— Commentary —
I think of early stage investing like I think about counting cards in Blackjack. You can’t guarantee a win on any given hand, even if you have an edge, but if you do have an edge, you can win more than you lose over the long term.
I use that analogy a lot when evaluating robotics deals. I’ve invested in several robot companies over the years and while I am very bullish on the prevalence of robots and the work they do down the road, I’m less bullish on the ability of an investor to make money in the space.
Most robots are built to be direct labor replacements. There is a task that is done by a human and now a robot will do it. This is happening in many areas of manufacturing and agriculture, and will continue to accelerate through all types of physical labor. But knowing many jobs will be replaced by robots doesn’t make it easy to pick winning companies. Why? Because when robots are just direct labor replacements, they rarely have any special defensibility or competitive advantage and thus, the winning companies tend to be the ones who had the best luck or timing, or simply raised the most money the fastest.
Contrast that with other types of robotics companies that enable things that were not possible before. Think of robots that do tasks and, as they do them, scan and monitor the world they inhabit to collect data at levels not done before. Now we don’t just have automation - we have a sensor to provide real-time data and a new data set to use for other tasks.
Let’s take an example. Say Blue Apron decided that not only would it sell you a meal kit, it will also sell you a robot that opens the door, picks up the meal kit, goes to the kitchen, and prepares it. This saves you time. But is it a defensible business? To answer that I’ll ask - how difficult is it to change out the robot to a HelloFresh robot? Probably not that hard.
Now take a robot that replaces a farm laborer. Not only is the robot performing a task, but it’s inspecting the crops every day with cameras and sensors. This allows a data layer to be built on top of the robot’s work. This data layer gives the company another product to sell, and a more defensible position. Rolling up all of that historical data and correlating it to crop conditions at harvest is valuable. Those are the kinds of robotics investments I prefer to do.
This second kind of robotics company doesn’t always win. Building companies is hard and fraught with risk, and so is investing in them, but, it provides a more likely chance at a nonlinear outcome. And while the first type of robotics company will have some winners too, I just find that for me, who will win is more difficult to predict.
In investing, you have to play where you have an edge, and I don’t see an edge in direct labor replacement robots.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/the-impact-of-initial-data-set-tradeoffs,"Happy Sunday and welcome to Investing in AI. I’m Rob May, a partner at PJC investing in AI and robotics companies, most recently Deeplite. I also host the Investing in AI podcast. The most recent guest was Rana El Kaliouby, CEO of Affectiva. It’s a great episode if you have the time to listen.
— Interesting Links —
Integrating Knowledge Graphs With Pre-Trained Corpa. Google AI Blog.
Deepfake Satellite Imagery As A Threat. The Verge.
Balancing Performance Capacity and Budget For AI Training. Nextplatform.
How a Startup Beat Healthcare Heavyweights In An AI Contest. Statesman.
The State of AI in 15 Graphs. IEEE Spectrum.
— Research Papers —
Social Behavior Understanding Using Deep Neural Networks. Link.
Texture Generation With Neural Cellular Automata. Link.
Federated AI for Unified Credit Assessment. Link.
— Commentary —
In 2016 I was an angel investor in an AI company (which will remain nameless for now) with a strong “human in the loop” model. Several VCs called me to ask why I had invested and they all had the same complaint. “It’s just a services company. The gross margins are only 37%.” I argued for some vision for where it could go because, the company was collected data on what these humans did and was building a data set to automate many of their tasks.
Fast forward 4 years later and the company has 8 figure revenue, a 9 figure valuation, and 80% gross margins - like a software company. The model worked. Collecting the data took time. Now the business is more defensible than ever.
This particularly entrepreneur was able to push through and raise money because of his track record, despite the then-skepticism of his business model. But this issue about data has plagued many startups and projects at large companies as well.
In a world where data matters more than ever, companies large and small face a tradeoff. Most projects require some sort of early proof points to get and keep financing. Few can wait 4 years to show the return this startup did. But, defensibility in AI often comes from unique data sets, which take time to build.
Value in business usually comes from defensibility, but in our short-term-thinking world, few may have the patience to wait on AI defensibility. If you, as an entrepreneur, CEO, or product manager, have to make the tradeoff between early business traction and longer term defensibility, how do you think about the data aspect of that?
Ideally you can find a path that cuts through both - with some near term wins and a type of defensibility that grows linearly with your customer traction, but also starts to gain unique data and lay the groundwork for more unique, and defensible, data sets later on. But what do you do when you can’t?
Contrary to conventional wisdom that AI may be the “sport of kings,” I think this presents a unique opportunity for startups to disrupt some large company AI projects. While large companies have more data and more compute, most lack the patience to invest in longer term initiatives. If acquiring the data set to build something uniquely defensible takes several years, I think private capital, more than public markets, are more open to those business models and the timeframe required to build them.
Because investing in data, labeling, and training, all takes time, then by the time others realize it’s working in a certain area, it is too late to launch a competing project.
If you work for a big company, you should highlight for your team that the defensibility dividend pays very well. These investments, while slow to mature, are worth making. And if you are in a startup, your best bet of raising is to articulate the long term value of the defensbililty you are building.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/the-rise-of-prompt-engineering-and,"Happy Sunday and welcome to the Investing in AI Newsletter. I’m Rob May, a Partner at PJC, focused on AI and Robotics. Be sure to check out the latest Investing in AI podcasts on iTunes, featuring tech columnist Kevin Roose of the NYTimes discussing automation and Davis Sawywer of Deeplite discussing edge AI.
I want to point out a super interesting thread from a week ago. Karl Higley, who works on recommendation systems at Nvidia, wrote about how recommenders can collapse in on themselves by using data from the output of a recommender. It’s a big problem that few people are thinking about, but when AI makes recommendations, and those AI outputs become inputs into a new AI system, you can lose some of the data diversity which can cause issues.
Karl Higley 
@karlhigley
Counter-intuitively, if your recommender is trained on data collected from past interactions with recs, retraining new models regularly doesn’t resolve the issue of performance decaying over time and can actually make it worse. Here’s why...
1:48 PM ∙ Apr 7, 2021
— Other Interesting Links —
The state of mental health apps in AI. The Cut.
A gaming site reviews Intel’s AI powered hate speech censoring tool. Kotaku.
A critique of machine learning for finding accounting fraud. EconJournal.
ARM’s new architecture explains why NVIDIA is buying them. NextPlatform.
Geoff Hinton on what’s next for AI. MIT Tech Review.
State of the art AI models for every field. Towards AI.
— Commentary —
Want to hear about a new job title you might see popping up at some companies? Try “Prompt Engineer.” It’s a new way to think about interacting with AI, and comes from the launch of OpenAI’s GPT-3 model.
If you aren’t technical, GPT-3 is the world’s top language model for natural language processing, at the moment. The way you access GPT-3 is with an API, which is like a technical pipe that goes into the model and gets things back out. While APIs have been around for a long time, they’ve become pretty standardized. You ask the API for certain data, like the balance a customer owes, and that data gets returned. With GPT-3 it’s different.
GPT-3 requires a “prompt.” This prompt is something that tells GPT-3 how much language to give you, and in what format. It’s less precise than a traditional API because you aren’t necessarily saying “give me 14 words to finish this sentence.” But, you may want GPT-3 to give you enough words to finish something that sounds like it makes sense, within a range. And setting a definite limit may lead to an awkward outcome. Or if you are doing something like asking GPT-3 to write a Haiku, maybe you do need a very precise limit.
Here is an example of a prompt that shows the first sentences provided by the prompt writer and then GPT-3 finishing it off.
Prompt engineering is a little bit art and a little bit science. But it’s quickly becoming an important skill. The essence of prompt engineering is getting the model to give you the output you want in the format you want. It’s like playing a game with a human where you lead the human into what their natural language output to something should be.
For that reason, I wonder if we will see some lawyers turn to GPT-3 prompt engineers. If I think of one job that has the core skill of getting people to say what you want them to say, leading them along with prompts, it’s trial lawyers.
And I expect prompt engineering to grow. As huge powerful ML models like GPT-3 proliferate in all kinds of domains, these models will be smart and flexible, and getting the right data out of them will be a bit trickier in some cases than the skills required to read a database table or ping an API.
We often hear that automation will take jobs. And GPT-3 will certainly reduce the number of writers we need, but it, and tools like it, will increase the number of Prompt Engineers we need to make these tools work. It’s definitely something to watch.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/will-ai-create-the-first-trillion,"Happy Sunday and welcome to the Investing in AI newsletter. I’m Rob May, a Partner at PJC. Don’t forget to check out my podcast interview with Bloomberg Beta Partner James Cham, and most recent one on synthetic data with the Synthesis.ai team.
This substack covers investing in AI, and topics related to that. When we get technical, it’s to explain how that technology filters up through products, business models, and competition in markets.
— Interesting Links —
Baidu’s Chip Unit Raises A Round. Reuters.
Gartner’s Magic Quadrant Cites a Glut of ML Innovation. Venturebeat.
The Pastry AI That Learned To Fight Cancer. New Yorker.
It’s Time For Global Treaties on AI. Brookings.
AI: Are We Doing It All Wrong? CNET.
— Commentary—
U.S. Steel was the first billion dollar corporation, in 1901, shortly after it IPO’d. Obviously a billion dollars nominally was worth a lot more compared to one billion today. Then it was 54 years before the first $10 billion dollar market cap company - GM in 1955. From there, IBM crossed $100 billion in 1987, then Apple crossed the $1 trillion mark in 2018.
As you can see, the numbers go up by an order of magnitude, and do so at a faster pace, although a pace that is somewhat gated by the size and growth of the world economy. That makes me wonder… when will we see the first trillion dollar IPO? It’s probably closer than we think, driven by several factors, including inflation, more startups growing faster, tools that make it easier to start and scale, and automation coming from AI. It’s possible AI will enable the first trillion dollar IPO.
The question is - what will it look like, and how could you identify that company today if said IPO is 12-20 years away? Here are the properties I think you look for in that company.
It will be in a brand new massive market. This is actually the point I’m least comfortable predicting, but I’ll say it anyway. Google and Facebook were new markets. Uber, another large tech IPOs, was sort of replacing and eating into taxis and other forms of transportation. It really depends on how strictly you define “new market.” But when looking at companies that could grow into a $1T market cap, you have to imagine a world where some new device, service, experience, etc becomes so common that almost everyone in the world does/has it.
It won’t be an obvious extension of existing tech. If it was, the big tech companies would do it. This will be something new that will come up on them too fast, or, run counter to their business models and thus make it tough for them to do.
It will make incredible use of AI powered automation. This will help it scale fast, with fewer employees than other companies did through similar growth curves. In my opinion, this is the area where companies are the most behind - applying AI to common workflows.
It will be built on more of an applied AI breakthrough than a theoretical one. It’s really rare for pure new tech companies to get big fast.
It will be profitable quickly. This is one thing that is changing about AI tech. While seed rounds will stay the same size, I think growth rounds will be smaller as companies need less capital to scale.
For a long time now I’ve believed that the window of 2018 - 2028 is the decade in which a company (or two) like this will be started. While the future is hard to predict, these are my thoughts on what a company should look like, conceptually.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/the-economics-of-ai-with-gpt-3,"Happy Sunday and Welcome to Investing in AI. I’m Rob May, a Partner at PJC. I focus on AI and have invested in companies like Mythic, Botkeeper, Root, and Synthesis. If you have an AI company raising at any stage, I’d love to talk with you.
Also check out our podcast. The second episode just went live and features Heath Terry from Goldman Sachs. Coming up are interviews with Rana El Kaliouby from Affectiva, and James Cham from Bloomberg Beta.
This week I’m writing about the economics of GPT-3, how it changes what you can build, the skill sets you need in your company to use it well, and the best way to think about where value accrues in the NLP AI value chain.
— Interesting Links —
Is China Emerging As The Global Leader in AI? Harvard Business Review.
Driving Model Performance With Synthetic Data. Synthesis AI Blog.
What Chip Startups Can Learn From Google’s TPU Team. NextPlatform.
How to Defeat a Boston Dynamics Robot in Mortal Combat. Vice.
Pentagon Funds Killer Robots. Washington Post.
Cisco Unveils Real-Time Translation for Webex. ZDNet.
The Billion Dollar AI Problem That Keeps Scaling. NextPlatform.
— Research —
Graph Time Convolutional Neural Networks. Link.
The Dota2 Bot Competition. Link.
— Commentary —
GPT-3 made waves when it came out last year from OpenAI. It has been considered the best AI language model currently on the market. Over the past 5-6 years many NLP startups have emerged to perform different tasks. With the release of GPT-3 there have been some who thought it’s game over for these startups. In my opinion though, the right way to think about GPT-3 is closer to a piece of platform infrastructure, more akin to AWS. That isn’t a perfect analogy but I will explain below why I see GPT-3 as the real beginning of opportunities in NLP, not the end.
GPT-3 can’t perform every language task. There are some limitations based on the data set it was trained on. For example, it can’t help with news related issues about new topics because they aren’t in the training data. And it also can’t generate really long pieces of text. An intro paragraph for a blog post is within it’s scope. A full blog post is not. The way to think about GPT-3 is that is can very rapidly generate language for common, short, language tasks.
If the barrier to generating basic language goes down, what goes up in value?
First of all, the skill of prompting GPT-3 goes up in value. The way you interact with the model is to give it various prompts - pieces of language that tell it what type of words and sentences to generate. There is an art to prompting GPT-3 and that type of “prompt engineering” can be valuable. For example, people are using GPT-3 to write code, solve equations, and other creative ideas that don’t seem at first glance to be NLP problems. Better prompts get better results, and prompt engineering is a valuable skill.
Secondly, user experience design for NLP workflows increases in value. One of the things I’ve seen apps doing with GPT-3 is generating several language samples of whatever it is you want - blog post titles, ads, short emails, etc. The workflow for approving those, changing those, declining those, is not something we are used to yet, so building a good UX is valuable at this point in the industry trajectory.
And finally, the value of vertical-specific NLP for language not well represented in GPT-3 will go up. The model wasn’t trained, for example, on a lot of highly technical medical language, so building applications that need that might be best served with combo of GPT-3 plus in-house language models.
One thing to keep in mind is that OpenAI requires you to meet regularly with them to discuss your application, and any changes you are making. To get more access as you scale also requires OpenAI approval, which could be a gating factor to rapid growth for apps built on GPT-3. So there is definitely a tradeoff between using GPT-3 to make language easy, and the possible limitations OpenAI may impose that drag on your business. In my opinion though, it’s worth the risk to build on the platform.
There are some interesting trends to watch in this space. Microsoft has a special license to GPT-3, so I’m curious to see where the OpenAI-MSFT relationship goes. Models are coming down in price to train for a given model size, so, while training GPT-3 from scratch is beyond the price point of most companies, in 2-3 years it might be much closer, and I expect to see GPT-3-like functionality available much more cheaply. Of course, by then we may have a GPT-4 that is the best of breed. Who knows?
The way to play the NLP space as an investor, then, is to focus on use cases that benefit from dramatically reduced costs of common language tasks. GPT-3 can’t yet replace a Forrester analyst or a specialist in any field, but tasks that are labor intensive for writing yet common enough that almost any educated human can perform them are good candidates to be done by GPT-3.
Value will accrue in the areas I mentioned above - savvy NLP UX design and workflows, vertical applications where GPT-3 struggles, and creative prompt engineering for unique use cases.
GPT-3 is a transformative piece of technology, and an important part of the emerging tech stack of AI. But it’s the beginning of a wave of startups and established companies using NLP, not the end. Expect to see use cases we haven’t even imagined yet.
Thanks for reading, and as always, please reach out if you have questions, comments, or other feedback.
@robmay"
https://investinginai.substack.com/p/why-amazon-will-become-an-ai-chip,"Happy Sunday and welcome to Investing in AI. I’m Rob May, a Partner at PJC investing in AI and Robotics companies. If you are new, this newsletter is about how AI impacts products, business models, and markets both public and private. The newsletter has 3 parts. The first part has some useful AI news links. The second part has a couple of research papers I found interesting. The third part is some commentary on something AI related. This week I’m suggesting Amazon will get into the chip market. Read on to understand why.
— News Links —
AI to Replace Humans In Cybersecurity. ZDNet.
GPT-3 Is Ready For Business. IEEE Spectrum.
AI Generated Shareholder Letter. Mine Safety Disclosures.
New DRAM Could Accelerate AI. IEEE Spectrum.
Spying Eyes Everywhere Now Share An AI Brain. Wired.
Learning To Reason Over Tables. Google AI Blog.
— Commentary —
Chip fabrication is a hot space right now, after a couple of decades of being not so hot. I know a lot about hardware because I started my career at Harris in June of 2000, as a FPGA and ASIC designer. I loved the work but, it was a time when most everything was moving into software, and hardware was an afterthought for most products.
At the moment, it seems like everyone is worried about our ability to produce chips, but it wasn’t always that way. In 1987, when TSMC was formed in partnership with Phillips and the Taiwanese government, chip production didn’t matter that much. The technology stage of a chip fabrication facility is called a “process node” and is usually denoted by a size. So when you hear people say 32nm or 15nm or 7nm, that represents the process node of that fabrication facility.
Some chips can use slightly older process nodes, but part of the reason the chip fabrication business is so capital intensive is that the leaders have to keep pushing the process node ahead to smaller and more power efficient chips. TSMC started two process nodes behind the market leaders, and took a decade to catch up. Now, 35 years later, TSMC leads the pack.
But for most of recent history, the software has mattered more than the hardware, so chip fabrication was not considered a strategic business as it has been for the last year or two. So, what changed?
Workloads changed.
For most of the history of AI, AI tools were programmed and run on CPUs like everything else. But sometime around 2010, as neural networks came into vogue, some smart people realized that a GPU, which is designed to process graphics in computers, worked much the way a neural network does, and they hacked the GPU to train their neural net. It was a huge success, and NVIDIA, the world’s top GPU manufacturer, embraced neural nets.
But AI workloads continue to change, and this has led to an explosion of interesting chips. You have neuromorphic chips, spiking neural net chips, analog crossbar arrays, chips using memristors, and much more. AI workloads run a bunch of small computations in parallel instead of the serial nature of most processing workloads, so all of these chips are figuring out how to run AI workloads most efficiently from a time, power, or performance perspective.
For 60 years, most of our chips have centered on the “von neumann” architecture. Even though there are other ways to do compute, we had standardized and pushed performance forward primarily via power and speed enhancements of the same architecture. Now AI is blowing up that paradigm.
This brings me back to Amazon. The reason to get into the chip business now is that AI chips, because they are functionally different, can perform well on AI workloads without using cutting edge process node technology. So if TSMC is pushing 3nm process node tech, you don’t need a new AI chip to be fabricated with that. You can use something older and more stable and it still performs very well. In fact, it will outpeform a CPU built at 3nm process node.
What this means is that there is older equipment available, that most chip companies can’t really use, that is very valuable to the AI chip companies. That means it is a unique time to get into the chip industry.
On top of this workloads-changing-equipment moment, there is the fact that chip fabrication facilities are seen as strategic national assets, which means you can probably get matching funds or tax credits or some kind of economic benefit from putting capital to work in this area, if you ask the government for it.
I think Amazon is well positioned to do this. Chip fabrication is a capital intensive business, which Amazon likes. There are probably financial incentives to enter the market. And it’s a unique time to be able to do this without being on the cutting edge of process node tech. Plus, it’s a way for Amazon to engage in more vertical integration for AWS.
It makes sense to me, so, I expect to see Amazon enter the chip business within the next 24 months. Stay tuned to see if it really happens.
That’s all for today. Thanks for reading.
@robmay"
https://investinginai.substack.com/p/where-does-value-accrue-in-the-edge,"Happy Sunday. I’ve re-named this substack Investing In AI for a few reasons. First, I’ve spent 5 years writing about the emerging AI ecosystem and trying to educate people on what is happening in AI. But now the most interesting thing to me is how the presence of AI impacts business models, ecosystems, companies, and markets. That’s what this newsletter will be about.
Secondly, I’m starting a podcast about Investing In AI, and already have some awesome interviews lined up.
The newsletter and podcast won’t just focus on startups, but the impact of AI on investing in public and private markets. It will focus less on new technology, and more on what the new technology means for markets and companies. As AI grows, I feel like this is the right focus for me going forward. I hope you still enjoy it. The format will still stay the same with some interesting links to read, and some commentary.
— Interesting Links —
AI is Both Miraculous and Dangerous. EE Times.
Chinese Lab Aims for Big AI Breakthroughs. Wired.
The First Bots Get Fined For Ticket Scalping. The Verge.
Gaze Estimation and GANs. Synthesis Blog.
Plasmonics. IEEE Spectrum.
Thoughts On Personalization Algorithms. Alve Mine Blog.
— Research Links —
A Multilingual Benchmark for Navigation Instruction Following. Link.
Fidelity and Privacy of Synthetic Medical Data. Link.
— Commentary —
Edge AI is going to be a big industry - bigger than cloud computing even, as everything everywhere gets some intelligence in it. The question to ask then, as an investor, is where will the most value accrue? Today I want to attempt to answer that by looking at 3 key areas of edge ai: chips, tools, and applications.
CHIPS - While you might think there are a lot of computer chips, there really aren’t. Particularly in the processor market, the core architectures are all pretty similar and the variations between products are small compared to the space of possible computer architectures. AI is changing that.
GPUs don’t solve all the problems because if you double the size of a model, it typically quadruples the time and power it takes to run on a GPU. As a result of the proliferation of AI workloads and different architectures (CNNs, DNNs, RNNs, etc), and different technical approaches to solving them (neuromorphic, analog crossbar, in-memory compute, spiking neuron, variations on CPU/GPU optimization) the result is fragmentation in the chip market when it comes to AI at the edge.
I think long term as tools standardize and workloads consolidate and architectures settle on best-of-breed that AI chips will consolidate quite a bit, but I think that’s 15-20 years away. If you think of investing in both public and private markets on tech cycles, the next two, maybe three tech cycles will still have major fragmentation at the chip layer.
That doesn’t mean they are bad investments. Chip companies are awesome when they win. While they take longer to get to market, and more capital to get to revenue, they are massively scalable, and can grow revenue even faster than software companies once they go. My take on investing in the chip market is that you can make money but you have to go deep and be really smart and make bets on which chips will win because there are A LOT of options.
TOOLS - The tools space for edge can be broken down into a few parts. The NN development tools like Tensorflow and Pytorch, which aren’t edge specific but do have features to help target edge devices. The MLOps space that includes companies like Verta doing model management. Although I don’t think any of them do edge models right now. There are companies that help design and compress models for the edge like Deeplite and OctoML.
These levels will, in my opinion, be oligopolies because they need to work with so many different ecosystem partners that once the first two or three companies hit sufficient support for many tools and chips, it won’t make sense for others to do it. The overall value accrued to this part of the stack versus the chip part of the stack may be higher because the fragmentation at the chip layer may cause price competition and these tools will be frequently used parts of key workflows.
APPLICATIONS - The applications layer will have a few new solutions, and many old solutions that have been upgraded with AI. Think of a security camera with built-in face recognition. At the application layer of the edge AI tech stack, the value accrue and the defensibility will most likely mirror that of the underlying market. For example, if automotive is competitive then AI automotive will be competitive. If security cameras are commoditized then AI security cameras will become commoditized.
There can be big investment wins at this layer, but, you need to find the winners of the pre-AI era that are moving rapidly into AI. Or, you need to find use cases where AI builds a flywheel that somehow takes a commodity market and brings some uniqueness to the first company to win the AI version of it. I don’t know what that would be off the top of my head, but I am keeping my eye out.
While there will be winning bets in all three layers, I think the tools layer is the most likely to accrue high value per company with the lowest overall investment risk. The chip layer will have big winners but will require a lot of research about use cases and workloads and what is getting early traction. And the application layer will mirror what happened in the pre-AI versions of those use cases.
That’s all for this week. Thanks for reading.
@robmay"
https://investinginai.substack.com/p/why-the-racial-and-gender-gap-in,"Update: This post caused a lot of consternation on twitter with people misinterpreting it. So for clarification, this post is not saying there is no bias in AI. And it is not saying that the lack of diversity in comp sci is a pipeline problem. This post says nothing about comp sci in general. This post says that when any field is new and small, you can’t hold it to the same diversity standards as something that is big and stable, because you can’t expect a nascent field to generate enough awareness to attract a diverse pipeline, until it hits critical mass of awareness. If you would like to criticize the arguments in this post, there are two ways to do so. You can dispute the claim that a nascent pipeline can’t be expected to be as diverse as a more mature pipline. I’m happy to discuss that. Or, you can argue that the last AI winter didn’t qualify as a nascent pipeline. But I don’t make any broader assertions about general tech pipelines here, and make no assertions about bias in the hiring process, so if you want to talk about those, then that’s fine but they don’t have anything to do with this post.
Happy Sunday and welcome to Technically Sentient. I’m Rob May, a Partner at PJC, specializing in AI and Robotics. In this issue I want to discuss something that not a lot of people seem to understand - that fixing gender and racial gaps in specific functional areas, like AI, takes time.
To understand why, we have to roll back the clock a couple of decades. The 1950s saw the invention of AI and a lot of excitement about the promise of the field. But since then, we have seen multiple AI winters - periods with little innovation or progress. In the mid 2000s, we were in one of these AI winters. It started to end in 2012.
The reason AI winters are important here is that, they influenced how many people studied AI. Take my own path as an example. I graduated with an Electrical Engineering degree in May of 2000 and took a job designing FPGAs and ASICs. In 2002 I decided to start a part-time Master’s degree in Computer Science, focused on AI. At the time, that mean doing mostly symbolic logic programming in LISP. I was disillusioned with the prospects of doing anything real with AI, so I quit the program halfway through.
For most of the time period of 2000 - 2015, if you had talked to most academic advisors or industry leaders about AI, the advice you would have been given was “stay away” or “don’t waste your time.” This was the general consensus. I remember visiting San Francisco in the summer of 2015 to give a talk at Jason Calacanis’ incubator about my experience building and selling my first company. That night I crashed at Jason’s house and told him my next company was going to be an AI company. “Really? Is there anything there?” That’s what the world’s greatest angel investor thought about AI in 2015. My point is, no one was paying attention to AI.
The reason that’s an important point is because it means there was no pipeline of AI talent. There were people working on AI who were passionate enough about it to ignore the naysayers, and there were people who had tangential skills that could easily be converted in AI engineers. But both were small groups of people.
Why wasn’t that small group of people evenly mixed among gender and race? Because when areas of study are small, the way you find out about them is more random and somewhat path dependent. So you are at the mercy of that randomness. It is what it is. And anyway, you were discouraged from studying AI no matter what your race or gender because it was a waste of time.
Now. In 2012 that started to change with the launch of AlexNet. This was the first neural network that made such a dramatic improvement for image classification that it woke up the AI industry again. It was 10 percentage points better than the nearest competitor. But even with that performance, it wasn’t like everyone started studying AI the next day. These things take time.
A few things happened next. First, the other people working in AI paid attention to see if AlexNet was a fluke, or was real progress. That led to huge interest in CNNs and image classification, and by 2015, the error rate on Imagenet was similar to human levels. That was the moment that the broader tech community began paying more attention.
So let’s recap. From 2000 - 2012 you have very few people in general in AI because it was considered a waste of time. From 2012 - 2015 you have a slight uptick in the influx by people who knew AI people and the results of AlexNet and other image classification successes. Forward thinking colleges began offering more AI classes, but not a ton. But from 2015 - 2017, that exploded.
Now, finally, we got the very beginnings of a broader group of people studying AI, and being encouraged to pursue it. Then in 2017, I think it was Carnegie Mellon that offered the first AI bachelor’s degree, although I can’t pin down the exact date. That act got the attention of other schools, which followed suit in the following years.
On top of that, more people in tangential fields rebranded themselves as AI people. Now, finally, we began seeing some small pieces of diversity in the field.
But think about this - the very first people to start AI degree programs haven’t graduated yet. And the people who got CS degrees with an AI focus from the schools that adopted AI classes early are just a year or two into the workforce. Getting a PhD in AI is probably 7-8 years from high school graduation, and getting the education and experience to have an impact in industry is simliar - 4 years of school and 3-4 of work experience.
My broader point here is - to evaluate whether there are gender and race issues in AI takes 10-12 years from the time the field finally got hot. Before 2015, women were discouraged from going into AI because EVERYONE was discouraged from going into AI. If we consider 2015 the industry breakout year, then I’d expect it to be 2025 before we see some semblance of a gender parity trend in AI just because you need time for people to see AI as an opportunity, study it, and get in a position to contribute.
So when people ask me about gender and race in AI, I encourage them to take a full pipeline view. Don’t just look at the change in researchers and academics year to year. Look at the growth in incoming college students interested in the field, and use that as a projection of where the field will be in the future.
Changing these things takes time. Progress can be slow. But, like the Buddhists say, “little by little one walks far.” We have far to walk, but demanding rapid major progress ignores the reality of how long these things take, and ignores the progress we are making little by little.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/the-economics-of-edge-ai-will-make,"Happy Thanksgiving Sunday and welcome to Technically Sentient. I’m Rob May, a partner at PJC. As an angel and a VC I’m invested in over 75 AI companies, and my goal with this newsletter is to discuss some of the things I’ve learned, and am learning, about how AI gets rolled out in the real world. This week I’ve written some commentary on Edge AI, which is a big investment thesis for me in 2021.
— Interesting Links —
Robots help with real estate sales. NYTimes.
Drift patents a new converational AI. Drift Blog.
GRANT is a collection of techniques for model explainability. Wagtail Labs.
Has China caught up to the US in AI? Forbes.
AI has a replication crisis. MIT Tech Review.
The forces driving sex robots. (safe for work) Dianaverse.
Facebook’s new AI misinformation detector. OneZero.
— Research Papers —
Adversarial Generation of Continuous Images. Link.
European Strategy on AI. Link.
Zero Shot Visual Slot Filling as Question Answering. Link.
— Commentary —
AI models use compute in two different forms - training, and inference.  The vast majority of compute workloads in the long term will be inference - e.g. I've trained a model to detect cats vs dogs, and now I just run it on the animals I see in the world to tell me if it's a cat or dog.  Training is very processor intensive and will most certainly be done in the cloud for now, but, AI edge inference will not because the economics don't make sense. 
If I train a model for a security camera to detect faces, or people with guns, or something like that, it doesn't make sense for me to constantly stream data back to the cloud, then run inference to see if the current scenes contain a face, then send the answer back to the camera.  It's much cheaper to buy a cheap microcontroller, deploy an edge AI model, and let it run continuously on the camera.  This hasn't been possible because running inference on the edge took too much compute, but several technologies are changing this.
There are companies working on this. At the chip level, I’m an investor in Mythic, an analog AI chip for lower power edge inference. I’m also an investor in Reality.ai, which makes it easy to deploy edge models for inference on data from physical sensors. But this market is expected to grow, and edge computing workloads will outstrip cloud workloads in a few years.
This is one of the biggest trends investors, and many entrepreneurs, are missing. Cloud has been the dominate compute paradigm for 15 years. It’s about to change, and while cloud will remain important, edge AI will provide massive opportunities for innovation and scale. It’s something to pay attention to, and if you are working on an edge AI company, I’d love to talk to you.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/why-network-effects-are-important,"Happy Sunday and welcome to Technically Sentient! I’m Rob May, a Partner at PJC specializing in robotics and AI investments. This bi-monthly newsletter is a collection of news, research, and commentary on the AI industry.
— Interesting Links —
AI powered virtual influencers are making real money. Bloomberg.
Can Lab-Grown Brains Become Conscious? Nature.
Detecting Covid-19 Through Cellphone Coughs Using AI. MIT News.
Machine Learning The News For MacroEconomics Forecasting. BankUnderground.
Mark Cuban is sponsoring AI bootcamps. WSJ.
Can AI learn common sense from animals? Venturebeat.
Rethinking Attention With Performers. Google AI Blog.
Yann Lecun on why GPT-3 is overhyped. Facebook.
Robotaxis: Where We Are: EETimes.
— Research Links —
Away From Trolley Problems And Towards Risk Management. Link.
Generative Adversarial Networks In Human Emotion Synthesis. Link.
— Commentary —
Building a cutting edge robotics company is difficult. We are at a moment in time where the inputs to a typical robot are changing rapidly. Sensors, arms, cameras, motors, are all getting way way better and way way cheaper at a very rapid pace. This means it suddenly becomes economically feasible to do things with a robot that you couldn’t do before. And it means robotics companies can be built with less capital than a similar company would have required in the past.
But because things are changing quickly, it sometimes presents a problem for robotics companies. Imagine you are building a busboy robot. It can clean off a table in a restaurant in 5 minutes. It’s a bit slow, but human busboys are hard to come by and don’t want to work for the minimum wage most restaurants pay. When humans clear the table though, it takes roughly 45 seconds.
Now say this robotic busboy company has raised $15M from venture capitalists, and can license it under a RaaS (Robots-as-a-Service) model for $250 per month. How should this company think about building a v2 of the robot that is faster and more functional versus selling more of a v1 robot? There are a few factors at play here:
A v2 would be faster, and possibly better. Say it could bus in 2.5 minutes, and has the added functionality of being able to load the dishes in the dishwasher.
General tech trends around sensors, motors, and other robot parts are advancing at rapid rates in quality, and quickly declining in price.
To raise more financing, the company needs to show product/market fit - that people like buying and using the robot.
From a business strategy perspective, this presents a problem. The busboy robot isn’t really defensible as a business model, particularly under a RaaS model. With all the rapid tech advances and declining costs, in 2 years someone can probably build a better robot for a lower price and compete directly with you. To counter that, you have to go build the v2 robot, and push the limits on every technology generation.
But that runs counter to your need to sell what you have and gain market penetration and revenue to continue financing the business. In this case, a first mover advantage is a problem as it validates the market and allows a fast follower to come in behind you and undercut your business. I’ve had this concern with many robotics companies I’ve evaluated for investment.
So how do you get around it? There are a few ways.
Engage a modest TAM. In general VCs like to invest in multi-billion dollar TAMs but, if you engage a smaller TAM that is still big enough to build a $100M revenue company, you may discourage competition because of market size.
Pre-emptively announce new products. When I worked in the computer chip industry, this was a common tactic. One company would announce ""in 18 months we will be releasing a wifi chip for mobile devices with XYZ power consumption and a ABC footprint.” This kind of foreshadowing caused other companies to evaluate if they really wanted to compete for the same market, given the long design cycles for hardware. And it can keep customers loyal if they like your brand and know better things are coming. But, be careful of pushing vaporware. If you are too far off on delivering, it can tarnish your reputation.
In some robotics markets, distribution can be an advantage. If you can find and own proprietary channels, that can work. I think this would be particularly beneficial in consumer robotics.
Patents may be able to help fend off competition in some cases but, my experience is that when tech fields are changing as fast as robotics is right now, defending patents is difficult. Companies can find ways to accomplish the same thing with other related tech.
Move away from a RaaS model to a one-time purchase model. Investors love recurring revenue streams so this may be tough, but, when someone has purchased a robot that still works, instead of licensing one month-to-month, they are less likely to replace it until they absolutely have to.
Develop robotic network effects, which I discuss below.
The best way to do this, in my opinion, is to look for robotics opportunities that introduce network effects, so that the purchase decision for the buyer is about more than just the task the robot does directly. I see three ways this can happen.
First, the learnings of one robot can be shared with another robot if they work in the same environment. Imagine if iRobot launched a laundry robot and a dish washing robot to go with the Roomba, and those robots could learn instantly from Roomba how to navigate your house. This kind of technology is a bit off but, will be possible soon.
Secondly, look for use cases where robots operate in small swarms. I’ve seen military combat drones, cleaning drones, and inspection robots for oil and gas tankers as examples of cheap disposable robots that work together in a swarm. Here the real intelligence is in the network of how the swarm behaves together, and that is much more difficult to replicate and the issue I first highlighted - better performance at lower cost because of advances in robotic parts - doesn’t matter.
And finally, look to data network effects from a robots as sensors out in the world. In many agricultural, logistics, and industrial applications I’ve seen where a robot uses machine vision and has a host of other sensors, that accumulated sensor data across locations and environments, and the way it impacts the nuance of robotic performance, becomes more valuable than the hardware of the robot itself. This is another way out of the problem that started this commentary.
The lesson here is, when starting a company in the robotics space, or considering an investment in one, you have to filter out standalone robotic applications that don’t have network effects or some other advantage beyond just being a great robot. Since the market is still early, there aren’t a lot of best practices around robotics businesses, and there are very few executives who know how to build and scale robot companies. Most of the companies are started by roboticists and as a result, many fall in love with parts of the technology or pet problems and don’t think through the larger implications of the business issues they may face.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/diseconomies-of-network-scale-and,"Happy Sunday and welcome to Technically Sentient. I’m Rob May, a partner at PJC, where I invest in seed stage AI and Robotics deals, and I write this newsletter to highlight things I’m reading and thinking about in the area of Intelligent Systems.
— Interesting Links —
NXP launches an AI ethics initiative for edge AI. EETimes.
Reinforcement learning simulates a fairer tax policy. MIT Tech Review.
An evaluation of named entity recognition models. Anno AI Blog.
Machine learning with tiny data. MIT Tech Review.
How Google uses AI to improve search. Venturebeat.
The era of analog AI computing has arrived. Mythic. (video)
Choosing the right AI business model. Medium.
— Research Papers —
Analogical and Relational Reasoning With Spiking Neural Networks. Link.
Do’s and Dont’s For Human and Digital Worker Integration. Link.
Self Imitation Learning In Sparse Reward Settings. Link.
— Commentary —
Albert Wegner wrote an awesome post recently on how Innovation Upends Extrapolation. The gist of the post is that it is dangerous to extrapolate a trend into the future when you are dealing with complex systems. One of the reasons for this that isn’t directly mentioned in the post is the idea of “diseconomies of scale.” Most students don’t learn this in business school - they just learn the idea of economies of scale. But diseconomies of scale are real. They arise because, at some level of scale, a system gets so large that it becomes tough to manage, and the costs of management make additional growth less economically attractive, or more difficult.
Diseconomies of scales are one reason that, despite all the concern in the mid 2000s that Walmart was going to own the world, they didn’t. Similarly, the concern in the 1980s that Sears would own the world, on in the 1940s that A&P would - were all wrong. Why? Diseconomies of scale eventually catch up to most physical systems.
Facebook, Google, and Amazon though, have gotten so big because diseconomies of scale are much rarer in digital systems with lower marginal unit costs and network effects. I don’t see the natural drag of diseconomies of scale slowing these businesses down. But Wegner’s post made me wonder if we will see a related concept for network effect businesses, something like “diseconomies of network density.” As networks scale, are there things that operate as natural attenuators on their growth and power?
Wegner’s post looked at the urbanization trend, but denser populations in certain areas can make the world ripe for a pandemic like covid. There are downsides to network density. Is the same true for digital networks?
Well, we see in all the talk about the election and social media that disinformation could be one attenuator on scaling network effects for certain types of networks. When barriers to publishing information were higher, and distribution was more local and contained, the value of pushing disinformation was low. Networks like Facebook and Twitter changed that incentive and value structure. And now the results are starting to affect the usage and growth of those tools.
This ties back to where things are going with intelligent systems because, on the one hand, intelligent systems might fix some of these problems for digital networks by working as auto-arbiters that make decisions about these things as good or better than humans. But as intelligent systems learn more, they could have their own new knowledge based networks, which could have their own problems too. And I wonder if there will be attenuation forces that will slow down their power, or if they will grow until they too are manipulated. Will there someday be “diseconomies of knowledge scale”?
These are issues we should think about as we build intelligent systems.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/why-isnt-there-an-ai-financial-crash,"Happy Sunday! This is Technically Sentient, a newsletter about AI and Intelligent Systems of all kinds. I’m Rob May, a Partner at PJC, where I invest in AI and Robotics. If you are raising an early stage round, please reach out.
This newsletter approaches AI from a slightly technical perspective, but more applied and business focused. I’m interested less in new techniques for squeezing better performance out of neural networks, and more what that technique would mean for applications, business models, and the economics of AI. Today in the commentary section I’m talking about financial capital vs production capital and why we haven’t seen an AI financial crash like we have in other areas. But first, let’s get started with reviewing some great links from the last two weeks and a few research papers.
— Links —
If you are only going to read one thing, read Nathan Benaich’s State of AI report. It’s excellent. State of AI.
The Future of Propoganda Will Be Computer Generated. The Atlantic.
Google Launches AI Platform Prediction. Venturebeat.
Facebook Rethinks AI Benchmarking with Dynabench. Facebook.
Can AI Model Economics Choices? Brookings Institute.
Object Detection With Synthetic Data. Synthesis AI Blog.
Is TOPS The Best Way to Measure AI Chips? Venturebeat.
Women AI Entrepreneurs Take Issue With Gender Bias. WSJ.
— Research —
Deep Learning For Time Series Classification. Link.
Adaptive Multi-Grain Graph Neural Networks. Link.
Emotion in Future Intelligent Machines. Link.
— Commentary —
I’m a huge fan of Carlota Perez’s work as laid out in her book “Technological Revolutions and Financial Capital: The Dynamics of Bubbles and Golden Ages.” The book lays out how financial capital is invested in new technologies during the “installation period”, often in speculative ways, but the overinvestment helps lay the ground work for the “deployment period” that comes after. In between there is normally a financial crash. The cycle is charted below.
The question is, why hasn’t there been an AI financial crash? Perez’s framework would imply that with the burst of AI investment in the 2015 - 2019 era, we should be in the middle of a big financial crash in AI companies. Yet, the failure rate so far seems to be in line with startups of all types, and we haven’t seen any pets.com style high profile AI companies crash.
My belief is in two parts. First - it’s coming in another 4-5 years for software/hardware. The issue with AI is that is came about using infrastructure that was already in place. The idea and mathematics of neural networks is several decades old, and GPUs to program there were in place for computer graphics. And of course everyone mostly used existing data sets, or pushed to get newer relevant data sets (which probably doesn’t qualify as financial capital overinvestment at those levels).
So I think the crash will come after the AI infrastructure changes that are coming finally get deployed. It starts with AI chips - new chips designed for new workloads. There are dozens of companies working on these and only a handful of them will win, but financial capital has chased the potentially stellar returns that 3-4 of these AI chip companies will provide.
Once the AI chips get to market, most of which work differently than the x86 chips we are used to, the tools and applications will change as well. While the chip companies have tried to use existing tool chains as much as possible, I expect to see a minor explosion in new tools 2-3 years after the first real production AI chips are in market, and a huge explosion in new applications and ideas 3-4 years after. That’s where the crash will come from.
AI chips and the relevant tooling are the infrastructure that is coming to really change AI from simple use cases like predictive analytics to more advanced and complex use cases like logic and reasoning. Applications will align with chips, and when a chip crashes it will take down big pieces of that ecosystem. But it’s coming.
The second thing I’d argue is that, if you include robotics as a form of AI (which is debatable, depending on the company and use case), I’d argue we are in the middle of a mini-bust. Several major robotics companies have failed in recent years (Jibo, Rethink both come to mind), and a few more are on the ropes. These have required a lot of investment, but have laid the infrastructure for a robotic boom by standardizing many of the parts for robotic systems.
The moral of the story then is, if you invest in AI, what we’ve had is almost like a pre-boom. The real boom and bust is still ahead of us. Be careful. And if you have thoughts on this issue, I’d love to hear them.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/technically-sentient-will-algorithms,"Happy Sunday and welcome to Technically Sentient. I’m Rob May, a Partner at PJC investing in AI and Robotics. If you have an early stage AI company, please send me a note if you are raising capital.
— Best Links of the Week —
Fake Data Could Help Solve Machine Learning’s Bias Problem. Slate.
LSTM’s are dying. What is replacing them? Towards Data Science.
Two Senators are proposing the U.S. seek a national artificial intelligence strategy. Homeland Prep News.
Algorithms to Live By. Nature.
Youtube Uses AI To Squash Conspiracy Theories. Wired.
—- Research —
Job2Vec: Job Title Benchmarking With Collective Multi-View Representation Learning. Link.
Large Scale Intelligent Microservices. Link.
Decoupling Representation Learning From Reinforcement Learning. Link.
— Commentary —
The Wall Street Journal ran an interesting article about how companies are determining who goes back to the office, and when. The thing that struck me about it is that some companies are using an algorithm to determine it, and there is a perception that the algorithm is somehow unbiased because it is driven by data.
It made me wonder if, in a world where algorithms run more and more of our lives, do we use them to abstain from assigning any responsibility to humans for the results, or to fix them? The whole reason for using an algorithm in this case is to get pass the buck a bit and avoid criticism. Given that humans don’t really like criticism, is this a canary in a coal mine for things to come?
Imagine a company that certifies algorithms against bias. You have a hiring algorithm that obtains that certification. Now when you reject a candidate, you just blame the algorithm, and they can’t sue for bias because the algorithm has been certified non-biased. Of course, this reminds me of the credit rating agencies rating CDOs as AAA ratings, and being very wrong. The same could happen here.
But I am less worried about that, and more worried about the implications of humans being increasingly out of the loop of responsibility. A lack of responsibility changes behavior, and that could take the whole AI industry in a bad direction - particularly if we believe things are less biased than they are because they meet some standard.
AI is too nascent to turn over final control of important decisions to AI even when it appears to work well. Until these algorithms have been vetted across many years of results, changing data sets, and different environments, they should be advisors to humans, not final arbiters. It sucks to get criticized, but better we have small errors from human mistakes than risk large scale errors from algorithmic ones.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/technically-sentient-the-coming-world,"Welcome to Technically Sentient! I’m Rob May, a Partner at PJC and former AI entrepreneur, and I write this newsletter to provide an overview and perspective of things happening in the AI market.
In 2015 I began writing Technically Sentient. That newsletter was rolled up into Inside.com and changed to InsideAI, and we started doing news too. Then it went daily. It’s been an awesome ride to 30,000 subscribers but the pace and format don’t match with how I’ve been thinking about AI, so while I continue to write commentary over there for now, I decided to go back here to a 2x per month newsletter that is more in-depth, analytical, and less news focused.
Also note - at PJC we are running an AI buyer’s pitch contest. Sign up here to pitch your AI company to customers, instead of VCs.
— Interesting Articles —
There is a great twitter thread on the reflexivity between AI algorithms and choice. Twitter.
A Guide For Responsible Use of Machine Learning APIs. Medium.
Winners and Losers at the Edge. SemiEngineering.
Graphcore, one of the most mature AI chip architectures, is going to a 3nm fabrication process, which is surprising. Anandtech.
— Research Papers of Note —
Grounded Language Learning, Fast and Slow. Link.
Fairness in the Eyes of Data: Certifying ML Models. Link.
Synthetic To Real Unsupervised Domain Adaptation. Link.
— Commentary —
I stumbled upon synthetic data from a robotics investment I made in 2016. The company was trying to build a robot to clean bathrooms and it turns out this is a very hard problem to solve. In fact, it turns out that training robots to be “smart” is a hard problem because there are a nearly infinite amount of situations they could find themselves in, and training them usually happens in the slow physical world, not the fast digital world. Having a robot learn to pick up various glasses may take weeks of training because, if the robot needs 10,000 examples to learn, maybe it can only pick up one glass every few seconds, so, it takes a while.
Contrast that with digital simulation, where a robot could learn 10,000 examples in seconds, digitally. Well, simulation has a long history in technology. When I started my career as a FPGA and ASIC designer, we always ran simulations of the chips and how they would work before actually programming them. Simulations were great, but the real world always uncovered some behavior the simulation didn’t.
Similarly, “synthetic data” is the term for data used to train robots via simulation. Combined with real world training, for most robotic use cases, the robots learn faster and have a higher level of accuracy, but they need both. But the rise of synthetic data has led to a more interesting meta trend - the rise of synthetic media.
The best known use case of synthetic media is Deepfakes, and my friend Rob Toews wrote about how deepfakes may wreak havoc on society. But there are also good use cases of synthetic media, like HourOne - a company that automates the creation of synthetic video characters, or Primer.ai - a company that auto-generates topical reports.
Instead of getting into the tech behind synthetic media, I want to start with the assumption that it keeps improving, and that the marginal cost of creating high quality content, video, audio, and text, continues to approach zero. And then I want to ask what happens?
When a bot can interview someone on a podcast, produce and post the whole thing, and market it, with almost no human intervention, then how much content gets produced, at what pace?
When anyone with a computer can create a video showing anything they want, in minutes - Joe Biden kissing Melania Trump, Putin and Trump in a fistfight - what kind of crazy content will we see? How will we know what to believe?
In 2007, some investors approached me about building a hyperlocal media company. I declined because, as I told them at the time, there are a limited number of ad dollars in the world, and if the content available to advertise across grows faster than those ad dollars then it will suck to be in the content business unless you are one of the biggest. I think that turned out to be largely true.
What does it mean, then, when we are on the cusp of a content explosion that not only means more content faster than ever before, but unlike the previous explosion this content may be very high quality, and often very confusing?
Attention is a constrained resource, and with more content competing for the same amount of attention (or, maybe the gross attention product of the world is slightly growing via multi-tasking and population growth and better internet access) then I expect the value of attention harvesting systems to go up. Systems that parse content and make recommendations will become even more valuable then they already are.
The value of distribution will also go up. If you have push channels into consumers, you will be in a powerful position. The media industry has flip flopped at times over business strategies that leverage the power of distribution or the power of content, sometimes integrating both via mergers, sometimes specializing in just one to be best of breed. But for this next wave, 2022- 2030, I think distribution wins.
The value of live events will go up - events that multiple people can simultaneously verify. Live events have mattered less because since the advent of Tivo and DVR, consumers have time shifted watching shows. For many times of content that will remain true, but for major events - or events where false content might matter, live broadcasting will become huge. I expect to see services that are livestreams, not of certain geographies or topics, but of important things happening right now that they can triangulate credibility because multiple people are sending in streams that all match. I see those things being a little bit like early cable news networks, covering whatever is happening right now, but with a different mission. It’s not the “nowness” that will matter, it’s the credibility the nowness provides. This will be a very lucractive business because of the aforementioned attention scarcity.
And finally, I expect digital worlds, metaverses, whatever you call them to explode. Eventually these will lead to a bot-economy where digital agents will transact and engage each other. First it will be on behalf of users but eventually they will become smart enough to do so on their own. So, if you look out to 2050 and beyond, there are probably videos created to target advertising to purely digital agents. Sounds crazy, I know, but, that’s where this is headed.
Those are my high level thoughts on synthetic media and the impact it has when marginal content creation costs approach zero. Soon I’ll do a deep dive on the tech and try to figure out how far out some of these things really are - technically. But if you are working on something in this space, please reach out. I’d love to chat.
Thanks for reading.
@robmay"
https://investinginai.substack.com/p/coming-soon,"Welcome to Investing In AI by me, Rob May. 2x Founder/CEO. Partner at PJC.vc specializing in AI and Robotics. University of KY alum.
Sign up now so you don’t miss the first issue.
Subscribe
In the meantime, tell your friends!"
